\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage[french]{babel}

\usepackage[default,scale=0.95]{opensans}
\usepackage[T1]{fontenc}
\usepackage{amssymb} %math
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{systeme}
\usepackage{bbm}

\usepackage{bookmark}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Titre},
    }
\urlstyle{same} %\href{url}{Text}

\theoremstyle{plain}% default
\newtheorem{thm}{Théorème}[section]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollaire}
%\newtheorem*{KL}{Klein’s Lemma}

\theoremstyle{definition}
\newtheorem{defn}{Définition}[section]
\newtheorem{exmp}{Exemple}[section]
% \newtheorem{xca}[exmp]{Exercise}

\theoremstyle{remark}
\newtheorem*{rem}{Remarque}
\newtheorem*{note}{Note}
%\newtheorem{case}{Case}



\title{Modélisation Statistique}
\author{Charles Vin}
\date{Date}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Introduction à la modélisation}
Un modèle est une simplification de la réalité, formelle grâce à des équations mathématiques conçus dans un but de prédiction. \\
La modélisation statistique s'appuie sur les résultats d'une expérience. Lorsqu'on répète l'expérience, le résultat peut varier. \\
Le but est de décrire le phénomène/processus à l'origine des données. Représenté par une variable aléatoire.  \\
Il y a deux rôles \begin{itemize}
    \item Descriptif : comprendre comment sont générés les observations, quel lien il peut y avoir entre les variables.
    \item Prédictif : pouvoir faire une prédiction pour une nouvelle date, lieu, individu...
\end{itemize}

\begin{figure}[!htbp]
    \centering
    \includegraphics*[width=.75\textwidth]{./Figures/fig1.png}
    \label{fig1}
\end{figure}

La modélisation statistique c'est \begin{itemize}
    \item Supposer que les observations $ x_1, x_2, \dots, x_n $ sont la réalisation de variables aléatoires $ X_1, X_2, \dots, X_n $.
    \item Donner une famille de loi possible F pour le vecteur aléatoire $ (X_1, X_2, \dots, X_n)$
    \begin{itemize}
        \item Si on peut écrire F sous la forme suivante $F = \{F_\theta , \theta n \theta \in \Theta \}$ avec $ \theta $ de dimension fini, on dira que le modèle est \textbf{paramétrique}. Par exemple $ F = \{P(\lambda ), \lambda , \lambda  \in \mathbb{R}^+\} $ .
        \item On considérera souvent que les variables $X_1, \dots, X_n$ sont \textbf{i.i.d.}. 
    \end{itemize}
\end{itemize}

L'échantillonnage est très important, sans contrôler l'échantillon on peut dire tout et son contraire. On ne vas pas vraiment s'attarder sur les méthodes l'échantillonnage mais il faut garder en tête que si celui-ci est mauvais, tout le reste, toute notre modélisation sera erroné. 

\section{La régression linéaire simple}
\subsection{Contexte}
On dispose d'un couple de variable aléatoire $ (X,Y) $.
\begin{thm}[de la variance totale]
    \[
        \underbrace{V(Y)}_{\text{Variance Totale}} = \underbrace{E(V(Y)|X)}_{\text{Variance résiduelle}} + \underbrace{V(E(Y|X))}_{\text{Variance expliquée}}
    .\]
    Donc 
    \[
        V(Y) \leq E(V(Y) | X)
    .\]
    Le fait de connaître $X$ permet de diminuer l'incertitude sur $Y$ . Ainsi $X$ pourrait servir à prédire $Y$. On pourrait prédire $Y$ par une fonction de $X$ : $ \hat{Y}= f(X) $.

    \begin{proof}[Preuve: ]
        \begin{align*}
            V(Y) &= E(Y^2) - E(Y)^2 \\
                &= E(E(Y^2)|X) - E(Y|X)^2 \\
                &= E(E(Y^2|X)) - E( E(Y)^2 | X) + E( E(Y)^2 | X) - E( E(Y)|X)^2 \\
                &= E( E(Y^2) - E(Y)^2 | X) + Var(E(Y|X)) \\
                E(Var(Y) | X)
        \end{align*}
    \end{proof}
\end{thm}

\begin{cor}[Résultat important admis]
    
    \[
        \arg \max E((Y - f(X))^2) = E(Y|X)
    .\]
    La meilleurs façon d'utiliser $ X $ pour prédire $ Y $ est de prendre $ f(X) = E(Y|X) $. On va alors poser 
    \[
        Y = E(Y|X) + \epsilon 
    .\]
    avec $ \epsilon  $ un terme d'erreur aléatoire. On peut montrer que \begin{itemize}
        \item $ E(\epsilon) = 0 $ 
        \item $ Cov(\epsilon, X) = Cov(\epsilon, E(Y|X)) = 0$ 
        \item $ V(\epsilon ) = (1- \frac{V(E(Y|X))}{V(Y)})V(Y) $ 
    \end{itemize}
\end{cor}

\subsection{Cadre de la régression linéaire}
Dans le cadre de la régression linéaire on va poser 
\begin{align*}
    &E(Y|X) = \alpha + \beta X \\
    \Leftrightarrow & Y = \alpha + \beta X + \epsilon 
\end{align*}
\begin{proof}[Preuve: ] (je sais pas trop ce qu'elle prouve là)
    Le point $ (E(X), E(Y)) $ appartient à la droite d'équation $E(Y|X) = \alpha + \beta X$
    
    \[
        E(E(Y|X)) = E( \alpha + \beta X) \Rightarrow E(Y) = \alpha + \beta E(X)
    .\]
    \begin{align*}
        E(Y) = \alpha + \beta E(X) \Rightarrow Y - E(Y) \\
            &= \alpha + \beta X + \epsilon - \alpha - \beta E(X) \\
            &= \beta (X - E(X) + \epsilon) \Rightarrow (Y-E(Y))(X-E(X)) = \beta (X-E(X)^2) + E(X-E(X)) \\
            &\Rightarrow E( (Y-E(Y)) (X-E(X)) ) = \beta E( (X - E(X))^2 ) + E(\epsilon (X-E(X)))_{=Cov(\epsilon,X) = 0} \\
        \beta &= \frac{Cov(X,Y)}{V(X)} = \rho_{X,Y} \frac{\sigma _y}{\sigma _x}
    \end{align*}
\end{proof}

\subsection{Application à la statistique}
On dispose d'un échantillon de taille $ n $ sur lequel on mesure deux variables que l'on nommera $ y_i $ et $ x_i $. 

On suppoera que le couple $ (x_i, y_i) $ 
DIAPO 17

\subsection{Exemple des salaires}
\subsubsection{Le modèle linéaire}
Quand elle demande d'écrire le modèle il faut écrire \begin{align*}
    &Y_i = \alpha + \beta x_i + E_i \\
    &E_i i.i.d., V(E_i) = \sigma ^2
\end{align*}

\subsubsection{Estimation de $ \alpha, \beta, \sigma  $ par les moindres carrés}
La méthode des moindre carrés ordinaire (MCO), consiste à choisir $ a,b $ qui minimise $ \sum_{i=1}^{n}(y_i - (\alpha +x_i \beta ))^2 $. On pose donc la fonction de coût suivante 
\[
    F(\alpha, \beta ) = \sum_{i)1}^{n}(y_i - \alpha + \beta x_i)^2
.\] 
    
\underline{Nouveau cours du 21/01} \\

\begin{note}[]
    y'a des belles images dans le diapo
\end{note}

Selon la méthode des moindres carrés ordinaire on estime $ \alpha  $ et $ \beta  $ par 
\[
    (a,b) = \arg \min _{(\alpha, \beta ) \in \mathbb{R}^2} F(\alpha ,\beta )
.\]

Pour trouver ça, on utilise la gradient comme l'année dernière :
\begin{align*}
    \frac{\partial F}{\partial \alpha } (\alpha; \beta ) &= \sum_{i=1}^{n} 2(-1)(y_i - \alpha -\beta x_i) = -2 (n \bar{y} - n \alpha - \beta n \bar{x})\\
    \frac{\partial F}{\partial \beta } (\alpha ; \beta ) &= \sum_{i=1}^{n} 2(-x_i)(y_i - \alpha -\beta x_i) = -2 (n \bar{xy} - n \alpha \bar{x}- \beta n \bar{x^2})
\end{align*}
avec $ \bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i, \bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_i, \bar{x^2} = \frac{1}{n}\sum_{i=1}^{n}x_i^2 $
\begin{align*}
                    &\nabla F(\alpha ,\beta ) = 0 \\
    \Leftrightarrow & \systeme*{
        \frac{\partial F}{\partial \alpha } (\alpha , \beta ) = 0,
        \frac{\partial F}{\partial \beta } (\alpha , \beta ) = 0
    } \Leftrightarrow \systeme*{
        \bar{y} = \alpha + \beta \bar{x} = 0,
        \bar{xy} = \alpha \bar{x} + \beta \bar{x^2} = 0
    } \Leftrightarrow \systeme*{
        a = \bar{y} - b \bar{x},
        \bar{xy} = (\bar{y} - b \bar{x})^{\bar{x}} + b \bar{x^2}
    }
\end{align*}
Focus sur la deuxième équation
\begin{align*}
                &\bar{x}\bar{y} = (\bar{y} - b \bar{x})^{\bar{x}} + b \bar{x^2} \\
    \Leftrightarrow & \bar{xy} - \bar{y}\bar{x} = b (\bar{x^2} - \bar{x}^2) \\
    \Leftrightarrow &b = \frac{}{} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) }{\sum_{i=1}^{n} (x_i - \bar{x})^2}
\end{align*}
\begin{rem}[]
    \begin{itemize}
        \item $ b $ est la version empirique de $\frac{Cov(X,Y)}{V(X)}$
        \item On pose $ y_i^* = a + b x_i = \bar{y} + b(x_i - \bar{x}) $, c'est l'estimation de $ E(Y_i | X = x_i) $ 
    \end{itemize}
\end{rem}

\subsubsection{Variance des estimateurs}

\begin{prop}[]
    $ a,b, y_i^* $ sont des estimation sans biais de, respectivement, $ \alpha, \beta, E(Y |X = x_i) = \alpha + \beta x_i $ 

    \begin{proof}[Preuve: ]
        ...j'ai un tableau de retard... j'ai pas $ E(B) $ gros gros calcul\\
        $ a $ est une réalisation de $ A = \bar{Y} - B \bar{X} $ 
        \begin{align*}
            E(A) = E(\bar{Y} - E(B) \bar{x}) &= E(\bar{Y} - E(B) \bar{x}) = \alpha + \beta  \bar{x}- \beta \bar{x} = \alpha  \\
            E(Y_i^*) = E(A + B x_i) = E(A) + E(B) x_i = \alpha + \beta x_i
        \end{align*}
    \end{proof}
\end{prop}

\begin{prop}[Gauss-Markov]
    Parmi les estimateur linéaires des $ Y_i, A,B $ sont de variance minimales. On dit que ce sont des estimateur BLUE (Best linear Unbiased Estimates). Ce sont les meilleurs estimateur linéaire que l'ont peut trouver en terme de variance.
\end{prop}

\textbf{Calcul de la variance des estimateurs}
\begin{align*}
    V(B) &= V(\frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) }{\sum_{i=1}^{n} (x_i - \bar{x})^2}) \\
        & \frac{1}{(2(x_i - \bar{x}))^2} V(\sum_{i=1}^{n} (Y_i \bar{y})(x_i - \bar{x}) ) \\
        &\text{ 2 tableaux de calculs horribles, voir moodle} \\
        & \frac{\sigma ^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \\
    V(A)&= V(\bar{Y} + \beta \bar{x}) = V(\bar{Y}) + \bar{x}^2 V(B) - \bar{x} Cov(\beta , \bar{Y})_{=0} \\
        &= \frac{\sigma ^2}{n} + \bar{x}^2 \frac{\sigma ^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \\
        &= \sigma ^2 ( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i - \bar{x}) ^2 }) \\
    V(Y_i^*) &= V(A + \beta x_i) = V(A) + x_i^2 V(\beta ) + 2 x_i Cov(A,B) \\
        &= \sigma ^2 (\frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2})
\end{align*}

\subsection{Le modèle linéaire Gaussien}
On pense que les erreur suivent une gaussienne pour pouvoir faire des tests 


\subsubsection{Exemple des salaires }
Voir diapo 

\underline{Nouveau cours du 28/01} \\
\subsubsection{Décomposition de la variance}
\begin{align*}
    SCT &= SCM + SCR \\
    \sum_{i=1}^{n}(y_i - \bar{y}) &= \sum_{i=1}^{n}( (y_i - y_i^*) + (y_i^* - \bar{y}) )^2 \\
    \sum_{i=1}^{n}(y_i - \bar{y}) &= \sum_{i=1}^{n}(y_i - y_i^*)^2 + \sum_{i=1}^{n}(y_i^* - \bar{y})^2 + 2 \sum_{i=1}^{n}(y_i - y_i^*)(y_i^* - \bar{y}) \\
\end{align*}
Or $ 2 \sum_{i=1}^{n}(y_i - y_i^*)(y_i^* - \bar{y}) = 0$ (remplacer $ y_i^* $ par $ a+bx_i $ et $ a $ et $ b $ par leur estimation $\rightarrow$ Pythagore

\subsubsection{Coeffficient de détermination $ R^2 $ }
\begin{align*}
    SCR &= \sum_{i=1}^{n}(y_i - y_i^*)^2 = \sum_{i=1}^{n}(y_i - a- bx_i)^2 \\
        &= \sum_{i=1}^{n}(y_i - ( \bar{y} - b \bar{x}) - bx_i)^2 \\
        &= \sum_{i=1}^{n}( (y_i - \bar{y}) - b (x_i - \bar{x}))^2 \\
        &= \sum_{i=1}^{n}(y_i - \bar{y})^2 + b^2 \sum_{i=1}^{n}(x_i - \bar{x})^2 - 2b \sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})\\
        &= S_y^2 + b^2 S_x^2 + 2b S_{xy}
\end{align*}
Or $ b = \frac{S_{xy}}{S_x^2} $, donc 
\begin{align*}
    SCR &= S_y^2 + \frac{S_{xy}^2}{(S_x^2)^2} S_x^2 - 2 \frac{S_{xy}}{S_x^2} S_{xy}\\
        &= S_y^2 (1 - \frac{S_{xy}}{S_x^2 S_y^2}) \\
        &= SCT ( 1 - R^2) \Rightarrow \frac{SCR}{SCT} = 1 - R^2
\end{align*} 
Or $SCM = SCT - SCR \Rightarrow 1 - R^2 = 1-r^2 \Rightarrow R^2 = r^2$

\subsubsection{Solutions}
Soient deux variable $ X $ et $ Y $ définies sur $ (\Omega, t, P) $ 
\[
    \rho _{X,Y} = \frac{E((X - E(X))(Y - E(Y)))}{\sqrt[]{V(X) V(Y)}} = \frac{E(XY) - E(X)E(Y)}{\sqrt[]{V(X)V(Y)}}
.\]
On dispose de $ n $ couple $ (x_i, y_i) $ iid et on suppose que $ (x_i,y_i) $ est une réalisation de $ (X,Y) $. L'estimation de Pearson de $ \rho _{X,Y} $ est
\begin{align*}
    r = \frac{ \frac{1}{n} \sum_{i=1}^{n} (x - \bar{x})(y - \bar{y})}{\sqrt[]{ ( \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2) (\frac{1}{n}\sum_{i=1}^{n} (y_i - \bar{y})^2 ) }}
\end{align*}

\subsubsection{Test d'analyse de la variance}
\subsubsection{Intervalle de confiance pour $ E(Y_j | X=x_j) $ }
\begin{align*}
    Y_j^* &= A + Bx_j \\
    V(Y_j^*) &= V(A + Bx_j) \\
            &= V(\bar{Y} - B \bar{x} + Bx_j) \\
            &= V(\bar{Y} + B(x_i - \bar{x})) = V(\bar{Y}) + (x_j - \bar{x})^2 V(B) \text{ car } B \text{ est non corrélé à } Y \\
            &= \frac{\sigma ^2}{n} + (x_j - \bar{x}) \frac{\sigma ^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2} = \sigma ^2 (\frac{1}{n} + \frac{(x_j - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2})
\end{align*}

\subsubsection{Intervalle de prédiction pour une nouvelle donnée}
\subsubsection{Validation des hypothèses du modèle}
\begin{itemize}
    \item $ y_i = \alpha + \beta x_i + E_i $ "les erreurs sont indépendantes
    \item ....................
\end{itemize}

\subsubsection{Graphique des résidus vs valeurs prédites}

\underline{Nouveau cours du 04/02} \\
full diapo sur les espaces euclidiens 

Dessin de projection dans onenote 

Toutes la fin dans OneNote 

\section{Régression linéaire multiple}
\begin{itemize}
    \item Est-ce que nos point sont aligné sur un plain
    \item Même hypothèses qu'avant
    \item On peut écrire $ \mathbb{Y} $  sont plusieurs forme (qu'on a vu avec la régression en DS)
    \item On utilise la méthode des moindres carrées again
\end{itemize}
\begin{exmp}[Exercice diapo]
    \begin{enumerate}
        \item $ X $ matrice avec $ n $ lignes et $ p+1 $ colonnes. Si le modèle est bien posé, toutes les colonnes de $ X $ sont linéairement indépendantes : $ rg(X^TX) = rg(X) = p+1 $ 
        \item $ X^T X \to (p+1 \times n)(n \times p+1) $.. $ (X^TX)^T = X^T X = $ Symétrique. \\ 
            Soit $ u \in \mathbb{R}^{p+1}, u^T X^T X u = (Xu)^T(Xu) = <Xu, Xu> = \left\| Xu \right\| ^2 > 0 $ 
        \item Si $ p=1 $ : \begin{align*}
            X^TX = \begin{pmatrix}
                1& \dots &1 \\
                x_{11} & \dots & x_{n1}
            \end{pmatrix} \begin{pmatrix}
                1 & X_{11} \\
                \vdots & \vdots \\
                1 & x_{n1}
            \end{pmatrix}
            = \begin{pmatrix}
                n & \sum x_{i1} \\
                \sum_{}^{}x_{i1} & \sum_{}^{}x_{i1}^2
            \end{pmatrix}            
        \end{align*}
        Si $ p=3 $ 
        \[
            X^T X = \begin{pmatrix}
                n & \sum x_{i1} & \sum x_{i2} & \sum x_{i3} \\
                ~ & \sum x_{i1}^2 & \sum x_{i1}x_{i2} & \sum x_{i1}x_{i3} \\
                ~ & ~ & \sum x_{i2}^2 & \sum x_{i2}x_{i3} \\
                ~ & ~ & ~ & \sum x_{i3}^2
            \end{pmatrix}
        .\]
        \begin{align*}
            X^T Y = \begin{pmatrix}
                1& \dots &1 \\
                x_{11} & \dots & x_{n1}
            \end{pmatrix} \begin{pmatrix}
                y_1 \\
                y_2 \\
                \dots \\
                y_n \\
            \end{pmatrix} = \begin{pmatrix}
                \sum
            \end{pmatrix}
        \end{align*}
        elle a effacé bruh, pourtant j'ai pas trainé
    \end{enumerate}
\end{exmp}
ESTIMATEUR DE $ \beta  $ A RETENIR PAR COEUR 
\[
    \hat{\beta }  = (X^TX)^{-1}X^T Y
.\]
Sa variance : 
\begin{align*}
    V (\hat{\beta }) = V((X^TX)^{-1}X^T Y) &= ((X^TX)^{-1}X^T)V(Y)((X^TX)^{-1}X^T)^T \\
        &= ((X^TX)^{-1}X^T) \sigma I_n (X (X^TX)^{-1}) \text{ car } V(AX) = AV(X)A^T \\
        &= \sigma ^2 (X^TX)^{-1}(X^T X)(X^T X)^{-1} \\
        &= \sigma ^2 (X^T X)^{-1}
\end{align*}

\subsection{Propriété de l'EMC}
\begin{thm}[Gauss-]
    Point (ii) : On montre que $ \forall \tilde{\beta }, \forall c \in \mathbb{R}^{p+1}, V(c \tilde{\beta } \leq V(c \tilde{\beta })) $ avec $ \tilde{\beta } $ un autre estimateur linéaire sans biais de $ \beta  $ \begin{itemize}
        \item $ \hat{\beta } $ est un estimateur linéaire $ \hat{\beta } = (X^TX)^{-1} X^T Y \to \hat{\beta } $ est bien conbinaison linéaire de $ Y $  
        \item $ \hat{\beta } $ est sans biais $ E(\hat{\beta }) = E((X^T X)^{-1}X^T Y) = (X^TX)^{-1}X^T E(Y) = (X^TX)^{-1} X^T X \beta  = \beta $ car $ Y= X \beta + E, E \sim \mathcal{N}(0, \sigma ^2 I_n), E(Y)= E(X \beta ) + E(E) = X \beta  $ 
    \end{itemize}
    Soit $ \tilde{\beta } $ un autre estimateur linéaire sans biais de $ \beta , \exists A : \tilde{\beta } = AY $ comme $ \tilde{\beta } $ sans biais $ AE(Y)=AX \beta = \beta \to AX = I_{p+1} $ \begin{align*}
        V(\tilde{\beta }) &= V(\tilde{beta} - \hat{\beta } + \hat{\beta }) = V(\tilde{\beta } - \hat{\beta }) + V(\hat{\beta }) + 2 Cov(\tilde{beta} - \hat{\beta }, \hat{\beta }) \\
        Cov(\tilde{beta} - \hat{\beta }, \hat{\beta }) &= Cov(\tilde{\beta}, \hat{\beta}) - V(\hat{\beta}) = A Cov(Y,Y) ( (X^TX)^{-1} X^T)^T - V(\hat{\beta}) \\
            &= \sigma ^2 A X (X^2X)^{-1} - V(\hat{\beta}) = 0
    \end{align*}
    Soit $ u \in \mathbb{R}^{p+1}$ \begin{align*}
        V(u \tilde{\beta}) &= u V(\tilde{\beta})u^T \\
            & u (V(\tilde{\beta } - \hat{\beta}) + V(\hat{\beta})) u^T \\
            &= V(u(\tilde{\beta} - \hat{\beta})) + V(u \hat{\beta})
    \end{align*} 
\end{thm}

\subsection{Etude des erreurs}
démo SCT = SCR + SCM
\begin{align*}
    \left\| Y - \hat{Y}1_{n} \right\|^2 = SCT &= (Y - \hat{Y})^T (Y - \hat{Y} 1_{n} )\\
    &= ( (Y - \hat{Y}) + (\hat{Y} - \hat{Y} 1_{n}))^T ( (Y - \hat{Y}) + (\hat{Y} - \hat{Y} 1_{n} )) \\
    &= (Y - \hat{Y})^T - (Y-\hat{Y}) + (\hat{Y} - \hat{Y} 1_{n})^T (\hat{Y} - \hat{Y}1_{n}) + 2 (\hat{Y} - \hat{Y}1_{n})^T (Y - \hat{Y}) \\
    &= \left\| Y - \hat{Y} \right\| ^2 + \left\| \hat{Y} - \hat{Y} 1_{n} \right\| ^2 + 2 ( X \hat{\beta } - \hat{Y} 1_{n})^2 (Y - X \hat{\beta }) \\
    &= SCR + SCM + 2 ( X \hat{\beta } - \hat{Y} 1_{n})^2 (Y - X \hat{\beta })
\end{align*}
\begin{exmp}[]
    $ H_x = $ hat matrix : $ H_x Y = \hat{Y} = X(X^TX)^{-1}X^T Y$ \begin{enumerate}
        \item \begin{align*}
            H_x * H_x &= X(X^TX)^{-1} X^T X(X^TX)^{-1} X^T \\
            &= X(X^TX)^{-1} I_{p+1} X^T = H_x \rightarrow \text{ projecteur } \\
%
            H_x^T &= (X (X^TX)^{-1} X^T)^T = X(X^TX)^{-1} X^T \rightarrow \text{ orthogonal} \\ 
%
            rg(H_x) &= J_n (H_x) \\ 
                    &= T_n ( X(X^TX)^{-1} X^T) \\
                    &= T_n (X^T X(X^TX)^{-1}) = T_n I_{p+1} = p+1
        \end{align*}
        \item $ H_x X = X (X^TX)^{-1} X^T X = X $ \\ 
        Chaque vecteur colonne de $ X $ appartient à $ Im(H_x) $. Or $ \dim (Im(H_x) ) = p+1 $ et les $ p+1 $ vecteurs colonnes de $ X $ étant indépendants, ils forment une base de $ Im(H_x) $. $ H_x $ est le projecteur orthogonal dans le sous espace vectoriel engendré par les colonnes de $ X $, on le note $ \mathcal{L}(X) $ \\ 
        Ainsi $ H_x Y = \hat{Y} $ donc $ \hat{Y} $ est le projeté orthogonal de $ Y $ dans l'espace engendré par les colonnes de $ X $ 
        \item Calcul matriciel 
        \begin{align*}
            (Y - \hat{Y})^T (\hat{Y} - \bar{Y}1_{n}) &= ( (1_{n} - H_x) Y )^T (H_x - J_n)Y \\
            &= Y^T (1_{n} - H_x)^T (H_x - J_n )Y 
        \end{align*}
        Or soit $ u \in \mathbb{R}^n $ 
        \begin{align*}
            J_n u = \dots, J_n u \in Im(H_x) = \mathcal{L}(X)
        \end{align*}
        Car la première colonne de $ X $ est le vecteur $ (1_n ,1_n \dots, 1_n) $. Donc $ (1_X - J_n)Y \in \mathcal{L}(X)$ et $ (1_n - H_x)Y \in \mathcal{L}(X) $ (J'suis pas sur de bien lire ce qu'elle écrit entre les 1 et les J)
    \end{enumerate}
\end{exmp}


\underline{Nouveau cours du 25/02} \\

\subsection{Le modèle étudié}
\begin{figure}[htbp]
    \centering
    \includegraphics*[width=.75\textwidth]{Figures/fig2.png}
\end{figure}


\[
    X = \begin{pmatrix}
        \text{Un} & \text{Poids} & \text{Age} \\
        1 & x_{1,1} & x_{1,2} \\
        \vdots  & \vdots  & \vdots  \\
        1 & x_{n,1} & x_{n,2} \\
    \end{pmatrix} , Y = \begin{pmatrix}
        Y_1 \\
        \vdots  \\
        Y_n
    \end{pmatrix}, Y = X \beta + E 
.\]
$ Y_i = $  pression artérielle pour le i ème individu \\
$ x_{i,1} = $  poids pour le i ème individu \\
$ x_{i,2} = $  age pour le i ème individu
\[
    \beta  = \begin{pmatrix}
        \beta _0 \rightarrow \text{ Intercept} \\
        \beta _1 \rightarrow \text{ Pente pour le poids} \\
        \beta _2 \rightarrow \text{ Pente pour l'âge} \\
    \end{pmatrix}, E = \begin{pmatrix}
        E_1 \\
        \vdots \\
        E_n
    \end{pmatrix}
.\]
Avec $ E_i \sim \mathcal{N}(0, \sigma ^2) $ iid. Finalement, $ E $  est un vecteur gaussien $ E \sim \mathcal{N}(0, \sigma ^2 I_n) $ \\
$ Y_1 \sim \mathcal{N}(\beta _0+ \beta _1 x_{1,1} + \beta _2 x_{1,2}, \sigma ^2 ) $ 

\subsection{Estimation de $ \beta  $ par la méthode MCO}
On a vu qu'on cherche à minimiser les $ \beta  $ (je crois) puis que finalement que la solution est (diapo 23/91)(a retenir par coeur)
\[
    \hat{\beta } = (X^T X )^{-1} X^T Y
.\]
et sa variance 
\[
    V(\hat{\beta }) = \sigma (X^TX)^{-1}
.\]
On a résumé les diapo suivante 

\begin{exmp}[Exercice important diapo 31/91]
    $ H_X $ matrice qui met le chapeau sur $ Y $ : $ \hat{Y} = X \hat{\beta } = X (X^TX)^{-1} X^T Y = H_X Y $ 
    \begin{enumerate}
        \item Il parrait qu'on a déjà fait l'exo elle a tout rappelé à l'oral 
    \end{enumerate}
    CCL de l'exercice : diapo 33/91
\end{exmp}

\subsection{Estimation de $\sigma ^2$}

\[
    \hat{\sigma ^2} = \frac{1}{n-p-1}\sum_{i=1}^{n} \hat{e_i^2} = \frac{SCR}{n-p-1}
.\]
\begin{enumerate}
    \item $ \hat{\sigma ^2} $ est un estimateur sans biais de $ \sigma ^2 $ indépendant de $ \hat{Y} $ 
    \begin{proof}[Preuve: ]
        Voir diapo, je la note pas 
    \end{proof}
    
    
\end{enumerate}
DIAPO j'ai décroché de ouf. on s'est arrêté sur le diapo 62/91 inclus

\underline{Nouveau cours du 04/03} \\
Loupé

\underline{Nouveau cours du 25/03} \\
Voir diapo

\end{document}