\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage[french]{babel}

\usepackage[default,scale=0.95]{opensans}
\usepackage[T1]{fontenc}
\usepackage{amssymb} %math
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{systeme}
\usepackage{bbm}

\usepackage{bookmark}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Statistiques décisionnelles},
    }
\urlstyle{same} %\href{url}{Text}

\theoremstyle{plain}% default
\newtheorem{thm}{Théorème}[section]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollaire}
%\newtheorem*{KL}{Klein’s Lemma}

\theoremstyle{definition}
\newtheorem{defn}{Définition}[section]
\newtheorem{exmp}{Exemple}[section]
% \newtheorem{xca}[exmp]{Exercise}

\theoremstyle{remark}
\newtheorem*{rem}{Remarque}
\newtheorem*{note}{Note}
%\newtheorem{case}{Case}

%add paragraph to table of content
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}


\title{Statistiques décisionnelles}
\author{Charles Vin}
\date{S6 2022}

\begin{document}
\maketitle

\textbf{\large Plan du cours}
\begin{enumerate}
    \item Rappel du 1er semestre
    \item Test d'ajustement : \\ 
        $ X_1, \dots, X_n $ va. iid. de loi $ \mathbb{P}_X $ 
        \begin{enumerate}
            \item Est-ce que les $ X_i $ suivent la loi $ L $ ($ \mathbb{P}_X = L $) ?  
            \item Est-ce que la loi des $ X_i $ appartient à une famille de loi ? Est-ce qu'il existe $ m, \sigma ^2 $ tel que $ X_i \sim \mathcal{N}(m, \sigma ^2) $ 
        \end{enumerate}
    \item Tests de comparaison : 
        \begin{itemize}
            \item Test non paramétriques : $ (\omega , \mathcal{F}, (\mathbb{P}_\theta )_{\theta \in \Theta}) $, On ne se restreint pas à une famille paramétrique de lois
            \item Tests de comparaison : $ X_1, ..., X_n $ jeu de données 1 et $ Y_1, ..., Y_n $ jeu de données 2. Les $ X_i $ $ Y_i $ ont-il même loi ? Les $ X_i $ et $ Y_i $ sont-ils indépendants ?
        \end{itemize}    
    \item L'ANOVA, voir cours de Mme Lavigne
    \item Etudes de cas
\end{enumerate}

\tableofcontents
\newpage






\section{Rappel sur les tests}
On fixe un modèle $ (\Omega , \mathcal{F}, (\mathbb{P}_\theta )_{\theta \in \Theta}) $.\\
On dit que le modèle est paramétrique s'il existe 
\[
    d \in \mathbb{N} \text{ tel que } \Theta \in \mathbb{R}^d
.\]
Sinon, on dira que le modèle est non-paramétrique. 
\begin{exmp}[de modèle paramétrique]
    \begin{enumerate}
        \item $ \Theta \subset \mathbb{R} \times \mathbb{R}, \mathbb{P}_\theta = \mathcal{N}(m, \sigma ^2), \theta = (m, \sigma ^2)$ 
        \item $ \Theta = [0,1], \mathbb{P}_\theta = Ber(\theta ), \theta \in [0,1] $ 
        \item $ \Theta = \mathbb{R}^+_*, \mathbb{P}_\theta = \mathcal{E}(\theta ), \theta \in \mathbb{R}^+_* $ 
    \end{enumerate}
\end{exmp}
\begin{exmp}[de modèle non-paramétrique]
    \begin{enumerate}
        \item $ \Theta = \text{ densité de probabilité sur } \mathbb{R}, \mathbb{P}_f = \text{ la loi de densité f}, f \in \Theta  $ 
        \item $ \Theta = \{(p_i)_{i \in \mathbb{N}}, \forall i \in \mathbb{N}, p_i \in [0,1] \sum_{+\infty}^{i=0} p_i = 1\}, \theta = (p_i)_{i \in \mathbb{N}}, \mathbb{P}_theta = \text{ la loi discrète tq} \forall k \in \mathbb{N}, \mathbb{P}(X=k) = p_k , $ 
        \item $ \Theta = \{ \text{ fonction de répartion de var.}\}, F \in \Theta,  \mathbb{P}_F = \text{loi de la va. dont la fonction de répartiton est } F, (\mathbb{P}_F)_{F \in \Theta } $ 
    \end{enumerate}
\end{exmp}

\begin{defn}[Test d'hypothèse]
    Soit $ \mathbb{X} = (X_1, \dots, X_n)$ un ensemble d'observations de loi $ \mathbb{P}_\theta  $  \\
    On appelle test d'hypothèse de $ H_0 $ contre $ H_1 $ (à $ H_0 $ et $ H_1 $ sont des sous-ensemble de $ \Theta  $). toute fonction des observations à valeur dans $ \{0,1\} $ \begin{itemize}
        \item à $ \phi (\mathbb{X}) = 0 $ correspond à conserver $ H_0 $ 
        \item à $ \phi (\mathbb{X}) = 1 $ correspond à rejeter $ H_0 $ au profit de $ H_1 $ 
    \end{itemize}
    $ R= \phi (\{1\}) $ est la zone de rejet, c'est l'ensemble des observation qui ... à un rejet de $ H_0 $ 
    \begin{rem}[]
        Si $ \phi (\mathbb{X}) = \mathbbm{1}_{h(\mathbb{X}) \in \mathbb{R}} $ on dira que $ h $ est la statistique de test et $ R $ la zone de rejet
    \end{rem}
    \begin{exmp}[]
        $ h(\mathbb{X}) = \sum_{i=1}^{n}X_i, R=[h, + \infty [ $. Test: $ \phi (\mathbb{X}) = \mathbbm{1}_{\sum_{i=1}^{n}X_i \geq k } $ 
    \end{exmp}
    \begin{exmp}[]
        $ \phi (\mathbb{X}) = 0 $ le test que conserve toujours $ H_0 $ est un test.
    \end{exmp}
\end{defn}

\begin{defn}[Erreur de première espèce \& Taille du test]
    l'Erreur de 1ère espèce est la fonction : \begin{align*}
        \alpha &: \Theta _0 \rightarrow [0,1] \\
                & \theta \mapsto \mathbb{P}_\theta (\phi (\mathbb{X}=1))
    \end{align*}
    La taille du test $ \phi  $ est 
    \[
        \alpha ^* = \sup_{\theta \in \theta _0} \alpha (\theta)
    .\]
    On dit que $ \phi  $ est de niveau $ \alpha $ si 
    \[
        \alpha ^* \leq \alpha 
    .\]
    Une suite de test $ (\phi _n)_{n \in \mathbb{N}} $ est de niveau asymptotique $ \alpha  $ si 
    \[
        \limsup _n \alpha _n^* \leq \alpha 
    .\]
    En général on a : $ \lim_{n \to \infty} \alpha ^*_n = \alpha $
    \begin{rem}[]
        Pour l'erreur de 1ère espèce le meilleur test est $ \phi (\mathbb{X})=0 $. En effet $ \forall \theta \in \Theta _0, \mathbb{P}_\theta (\phi (\mathbb{X}) = 1 ) = 0 $ 
    \end{rem}
    \begin{rem}[Cours de M.Thiam, def 12]
        Si vous préférez la formulation du 1er semestre, c'est tout aussi valable.
    \end{rem}   
\end{defn}

\begin{defn}[Erreur de seconde espèce et puissance]
    La fonction erreur de 2nd espèce d'un test $ \phi  $ est 
    \begin{align*}
        \underline{\beta} :& \Theta_1 \rightarrow [0,1] \\
                & \theta \mapsto \mathbb{P}_\theta (\phi (\mathbb{X} = 0))
    \end{align*}
    C'est la probabilité de conserver à tort $ H_0 $. On appelle en général erreur de seconde espèce la quantité $ \beta = \sup_{\theta \in \Theta _1} \underline{\beta } (\theta ) $ 
    
    La fonction puissance $ \gamma  $ est $ 1 - \underline{\beta }  $.

    \begin{exmp}[]
        Le test $ \phi (\mathbb{X}) = 0 $ (le test stupide) a une erreur de seconde espèce qui vaut 1. 
        \[
            \mathbb{P}_\theta ( \phi (\mathbb{X}) = 0 ) = 1
        .\]
        et sa puissance vaut $ 0 $ 
    \end{exmp}   
\end{defn}

\begin{defn}[p-valeur]
    Si pour tout niveau $ \alpha $, on a construit un test $ \phi _\alpha  $ Soit $ \mathbb{X} $ une observation. 
    \[
        p(\mathbb{X}) = \inf \{\alpha \in [0,1] \text{tel que } \phi _\alpha (\mathbb{X}) = 1\}
    .\]
    Si on choisit un niveau $ \alpha $ 
    \[
        \alpha < p(\mathbb{X}), \text{ on conserve } H_0
    .\]
    Et si $ \alpha \geq p(\mathbb{X}) $ on rejette $ H_0 $ 
\end{defn}

\begin{defn}[Test consistent]
    Une suite de tests $ \phi _n $ est dite consistent si pour tout $ \theta \in \Theta _1 $ 
    \[
        \gamma _n (\theta ) \to _{n \to \infty } 1
    .\]    
\end{defn}

\section{Tests d'ajustement}
Le but de ce chapitre est de répondre à la question suivante : \\
Étant donnée un échantillon $ X_1, \dots, X_n $  et une loi de proba sur $ \mathbb{R} $ nommée $ \mathcal{L} $ 
\[
    \text{Est ce que les } X_i \sim \mathcal{L}
.\]

\begin{itemize}
    \item $ H_0 = $ les $ X_i $ ont pour loi $ \mathcal{L} $ 
    \item $ H_1 = $ les $ X_i $ n'ont pas pour loi $ \mathcal{L} $ 
\end{itemize}
Comment comprendre ce problème ? 
\begin{enumerate}
    \item En général, on peut utiliser les fonction de répartition. La question devient $ F_X = F $ contre $ F_X \neq F $ (en tout point de $ \mathbb{R} $ )
    \item Si les $ X_i $ sont à support dans $ \{1,..., K\} $. La question devient $ \forall i \in \{1,..., K\}, \hat{p_i} = p_i $ contre $ \exists i \text{ tq } \hat{p_i} \neq p_i $ où $ \hat{p_i} = P(X=i) $ et $ p_i = P(L=i) $ 
\end{enumerate}
Énorme problème : On ne connaît pas la loi des $ X_i $, on connaît juste $ n $ réalisations. 

Problème plus difficile : Ajustement à une famille de lois ? Est-ce que les $ X_i $ proviennent d'une loi normale ? (sans en connaître les paramètres)
\begin{rem}[]
    Cette question est fondamentale pour valider un modèle
\end{rem}

\subsection{Le test d'ajustement de Kolmogorov-Smirnov}
\subsubsection{Rappels}
\begin{defn}[Fonction de répartition]
    Soit $ X $ une variable aléatoire réelle, sa fonction de répartition est la fonction \begin{align*}
        F_X :& \mathbb{R} \to [0,1] \\
            & t \mapsto P(X \leq t)
    \end{align*}
    Elle caractérise la loi de $ X $. \\
    Si $ X $ est à densité, $ F_X $ est continue. Les discontinuité de $ F_X $ sont les valeurs $ t_0 \in \mathbb{R} $ tel que $ P(X = t_0) > 0 $. 
    \begin{exmp}[]
        \begin{itemize}
            \item Si $ X \sim Unif({0,1}) $ 
                \[
                    F_X(t) = P(X \leq t) = \int_{0}^{t} \mathbbm{1}_{[0,1]}(x)dx = \systeme*{
                        0 \text{ si } t \leq 0,
                        t \text{ si } t \in [0;1],
                        1 \text{ si } t \geq 1
                    }
                .\]
            \item Si $ X \sim \mathcal{E}(\lambda ) $ 
            \[
                F_X(t) = \int_{0}^{t}\lambda e^{-\lambda x}dx = \systeme*{
                    0 \text{ si } t < 0,
                    1 - e^{- \lambda t} \text{ si } t \geq 0
                }
            .\]
            \item Si $ X \sim \mathcal{B}(p) $ 
            \begin{align*}
                F_X(t) = \systeme*{
                    0 \text{ si } t<0,
                    {1-p} \text{ si } t \in [0;1[,
                    0 \text{ si } t \geq 1
                }
            \end{align*}
        \end{itemize}
    \end{exmp}
\end{defn}

\begin{defn}[Pseudo inverse de la fonction de répartion]
    Soit $ X $ une var. de fonction de répartition $ F_X $. On pose 
    \begin{align*}
        F_X^{-1} :& ]0,1[ \to \mathbb{R} \\
                &x \mapsto \inf \{t \in \mathbb{R}, F_X (t) \geq x\}
    \end{align*}
    On l'appelle inverse généralisé de $ F_X $ et elle coincide avec l'inverse si $ F_X $ est bijective.

    Elle vérifie la propriété fondamentale 
    \[
        \forall x \in ]0,1[, \forall t \in \mathbb{R}, F_X^{-1} \leq t \Leftrightarrow x \leq F_X(t)
    .\]
\end{defn}

\begin{thm}[]
    Soit $ X $ une var. de fonction de répartition $ F_X $ et une variable uniforme $ U $ sur $ [0,1] $ alors 
    \[
        X \text{ et } F_X^{-1}(U) \text{ ont même loi}
    .\]

    \begin{proof}[Preuve: ]
        Soit $ t \in \mathbb{R} $ 
        \[
            P(F_X^{-1}(U) \leq t) = P(U \leq F_X(t)) \text{ comme } \{F_X^{-1} (U) \leq t\} = \{U \leq F_X(t)\}
        .\]
        Or $ F_X(t) \in [0,1] $ donc 
        \[
            P(U \leq F_X(t)) = F_X(t)
        .\]
        Ainsi $ F_X^{-1} $ et $ X $ ont la même fonction de répartition et donc la même loi
    \end{proof}
\end{thm}

\underline{Nouveau cours du 20/01} \\

\subsubsection{Le test de Kolmogorov-Smirnov}

\textbf{But} : Si on a $ X_1, \dots, X_n $ observation iid. Est-ce que la fonction de répartition des $ X_i $ est une certaine fonction $ F_L $ \textbf{donnée} ? \\
$ \Leftrightarrow F_X = F_L \Leftrightarrow $  La loi des $ X_i $ est la même que L \\

\begin{exmp}[]
    Se demander si les $  X_i \sim \mathcal{E}(1) $ revient à demander : Est-ce que $ \forall t \in \mathbb{R}, F_X(t) = (1 - e^{-t}) \mathbbm{1}_{t \geq 0} $ 
\end{exmp}

\textbf{Autre reformulation:} Est-ce que mes observations sont cohérentes avec l'hypothèse $ F_{X_i}= F $ ? Il va donc falloir estimer $ F_{X_i} $ et la comparer à $ F $ 

\begin{defn}[Fonction de répartition empirique]
    Soit $ X_1, \dots, X_n $ un échantillon iid. On appelle \textbf{fonction de répartition empirique} de $ X_1, \dots, X_n $ la fonction 
    \begin{align*}
        F_n :& \mathbb{R} \to [0,1] \\
            & t \mapsto \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{X_i \leq t}
    \end{align*}
    Illustration graphique \ref{fig1}:
    \begin{figure}[!htbp]
        \centering
        \includegraphics*[width=.75 \textwidth]{fig1.png}
        \caption{Exemple de fonction de répartion empirique}
        \label{fig1}
    \end{figure}
\end{defn}

\textbf{Rappels : }
\begin{enumerate}
    \item $ \forall t \in \mathbb{R}, F_n(t) \to ^{p.s}_{n \to +\infty } F_{X_1}(t) $ 
    \item De plus $ \forall t \in \mathbb{R} $ fixé 
    \[
        \frac{\sqrt[]{n}}{\sqrt[]{F_X(t) (1 - F_X(t))}} (F_n(t) - (F_X(t))) \to ^\mathcal{L}_{n \to +\infty } Z \text{ de loi } \mathcal{N}(0,1)
        .\]
    Ce n'est rien d'autre que le TCL pour la suite de variables iid. $ (Y_i = \mathbbm{1}_{X_i \leq t})_{i \in \mathbb{N}} $ 
\end{enumerate}

\begin{thm}[Glivenko-Cantelli]
    $ (X_i)_{i \in \mathbb{N}} $ une suite de va. iid. alors 
    \[
        \sup_{t \in \mathbb{R}} \left| F_n(t) - F_{X_1}(t) \right| \to^{p.s}_{n \to + \infty } 0
    .\]
    Illustration graphique \ref{fig2}: 
    \begin{figure}[htbp]
        \centering
        \includegraphics*[width=.75 \textwidth]{fig2.jpg}
        \caption{Illustration graphique de Glivenko-Cantelli}
        \label{fig2}
    \end{figure}
\end{thm}

Ce théorème montre que la bonne quantité pour savoir si $ F_X = F $ à $ F $ est une certaine fonction donnée est
\[
    h(F_n, F) = \sup_{t \in \mathbb{R}} \left| F_n(t) - F_{X_1}(t) \right|
.\]
\begin{itemize}
    \item Si $ F=F_X $ alors d'après le théorème de Glivenko-Cantelli :
    \[
        h(F_n, F) \to^{p.s}_{n \to + \infty } 0
    .\]
    \item Si je me suis trompé et que $ F \neq F_X $, alors 
    \[
        h(F_n, F) \to^{p.s}_{n \to + \infty } \sup_{t \in \mathbb{R}} \left| F_n(t) - F_{X_1}(t) \right|
    .\]
    En effet $ F_n \to F_{X_i} $ donc \begin{align*}
        h(F_n, F) &= \sup_{t \in \mathbb{R}} \left| F_n(t) - F_{X_1}(t) \right| \\
                    & \to^{p.s}_{n \to + \infty } \sup_{t \in \mathbb{R}} \left| F_n(t) - F_{X_1}(t) \right| > 0
    \end{align*}
\end{itemize}
De manière informelle, on a envie de dire \begin{itemize}
    \item Si $ h(F_n, F) $ est petit alors $ F_X = F $ 
    \item Si $ h(F_n, F) $ n'est pas petit alors $ F_X \neq F $ 
\end{itemize}. \\

\paragraph{Comment calculer en pratique $ h(F_n,F) $} ? \\ 

Données : $ X_1, \dots, X_n $ des valeurs. $ F $ une fonction de répartition cible. \\
\textbf{But:} Calculer $ h(F_n,F) $ de manière pratique. à $ h(F_n, F) = \sup_{t \in \mathbb{R}} \left| F_n(t) - F_{X_1}(t) \right| $ (Voir Figure. \ref{fig3})

\begin{figure}[!htbp]
    \centering
    \includegraphics*[width=.75\textwidth]{./fig3.png}
    \caption{Figure pour trouver la fonction $ h(F_n, F) $ }
    \label{fig3}
\end{figure}
\begin{note}[du dessin]
    Le but de cette explication est de montrer graphiquement et instinctivement pourquoi on ne regarde pas pour tout $ t \in \mathbb{R} $ mais uniquement à chaque saut.
\end{note}

\begin{enumerate}
    \item étape : avant $ X_(1) $ 
    \[
        \sup_{t \leq X_{(1)}} \left| F_n(t) - F_{X_1}(t) \right| = \max \{ \left| \frac{1}{n} - F(X_{(1)}) \right| , \left| F(X_{(1)}) - 0 \right| \}
    .\]
    On recommence pour les différentes valeurs de $ X_{(i)} $ et on voit que la plus grande distance entre les deux courbes est forcément atteinte à un des points de saut
\end{enumerate}

\begin{rem}[\textbf{attention}]
    Pour chaque saut, il faut regarder 2 valeurs AVANT et APRES le saut.
\end{rem}

Formule de calcul de $ h(F_n, F) $ 
\[
    h(F_n, F) = \max _{1 \leq i \leq n} ( \max ( \left| \frac{i}{n} - F(X_{(i)}) \right| , \left| \frac{i-1}{n}- F(X_{(i)}) \right|  ))
.\]
\begin{note}[]
    On fait le max pour tous les sauts du maximum entre la distance APRES (au moment du saut) et AVANT (juste avant le saut (i-1)).
\end{note}

\begin{exmp}[Cas concret]
    $ X_1 = 0.06, X_2 = 0.8, X_3 = 0.27, X_4 = 0.67, X_5 = 0.38 $ 
    \[
        F(t) = F_U(t) = \systeme*{
            0 \text{ si } t \leq 0,
            t \text{ si } t \in [0,1],
            1 \text{ si } t \geq 1
        }
    .\]
    Etape 1 : On ordonne les valeurs
    \begin{table}[!ht]
        \centering
        \begin{tabular}{|l|l|l|l|l|l|}
        \hline
            $X_{(i)}$ & 0.06 & 0.27 & 0.38 & 0.67 & 0.8 \\ \hline
            $F_n$ & 0.2 & 0.4 & 0.6 & 0.8 & 1 \\ \hline
            $F$ & 0.06 & 0.27 & 0.38 & 0.67 & 0.8 \\ \hline
            Après le saut : $\left| \frac{i}{n} - F(X_{(i)}) \right|$ & 0.14 & 0.13 & \textbf{0.22} & 0.13 & 0.2 \\ \hline
            Avant le saut : $\left| \frac{i-1}{n}- F(X_{(i)}) \right|$ & 0.06 & 0.07 & 0.02 & 0.07 & 0 \\ \hline
        \end{tabular}
        \label{tab1}
    \end{table}
    Ici $ h(F_n,F_U) = 0.22 $ 
\end{exmp}

\paragraph{Comportement théorique de $ h(F_n,F) $}

\[
    h(F_n, F) = \sup _{t \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{X_i \leq t} - F(t)  \right| 
.\]
est une variable aléatoire. 

A priori, la loi de $ h(F_n, F) $ dépend \begin{itemize}
    \item de $ n $ 
    \item de la loi des $ X_i $ 
\end{itemize}
\textbf{Rappel: } $ H_0: F = F_{X_o} $ contre $ H_1: F \neq F_{X_i} $ \\
Sous $ H_0 $ quel est la loi de $ h(F_n, F) $ ? 
\[
    h(F_n, F) = \sup _{t \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{X_i \leq t} - F_{X_1}(t)  \right| 
.\]
Soit $ U_1, \dots, U_n $ iid. uniforme sur $ [0,1] $ \\
Soit $ F_{X_1}^{-1} $ l'inverse généralisé de $ F_X $ \\
Alors $ F_{X_1}^{-1}(U_1), \dots, F_{X_1}^{-1}(U_n) $ ont même loi que $ X_1, \dots, X_n $. Ainsi en loi 
\[
    h(F_n, F) = \sup _{t \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{F_X^{-1}(U_i) \leq t} - F_{X_1}(t)  \right| 
.\]
Or $ \{F_X^{-1} \leq t\} = \{U_i \leq F_{X_1}(t)\} $ donc $ \mathbbm{1}_{F_X^{-1}(U_i) \leq t} = \mathbbm{1}_{U_i \leq F_{X_1}(t)} $ et donc 
\[
    h(F_n, F) = \sup _{t \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{U_i \leq F_{X_1}(t)} - F_{X_1}(t)  \right| 
.\]
Si $ F_{X_1} $ est continue, alors $  ]0,1[ \subset F_{X_1} (\mathbb{R}) \subset [0,1] $. Ainsi en reparamétrant le $ \sup $ on a 
\[
    h(F_n, F) =^{Loi} \sup _{s \in ]0,1[} \left| \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{U_i \leq s} - s \right| 
.\]
Dans cette formule, la loi de $ X $ (et sa fonction de répartition) n'apparaît pas ! \\
\textbf{Bilan : La loi de $ h(F_n, F) $ ne dépend que de $ n $ sout $ H_0 $ }

La loi de $ h(F_n, F) $ est tabulé pour toutes les valeurs de $ n $. On peut alors construire un test de niveau $ 1 - \alpha $

\paragraph{Le test de Kolmogorov-Smirnov à 1 échantillon \\}

Données : \begin{itemize}
    \item $ X_1, \dots, X_n $
    \item $ F $ une fonction de répartition continue
    \item $\alpha$ un niveau
    \item $ H_0: F_X = F $ contre $ H_1: F_{X_1} \neq F $
\end{itemize}
Soit $ h_\alpha $ le quantile de niveau $ 1 - \alpha $ de $ h(F_n, F) $
\begin{itemize}
    \item Si $ h(F_n, F) > h_\alpha $, on rejette $ H_0 $ 
    \item Si $ h(F_n, F) \leq h_\alpha  $, on conserve $ H_0 $ 
\end{itemize}
De manière formelle : $ \phi (\mathbb{X}) = \mathbbm{1}_{h(F_n, F) > h_\alpha } $ 

\begin{exmp}[retour sur l'exemple]
    Dans le tableau, on avait lu $ h(F_n, F) = 0.22, n=5 $. \\
    Test de niveau 90\% : la zone de rejet est $ h > 0.509 $ (d'après la table). Dans l'exemple on conserve $ H_0 $, les $ X_i $ proviennent d'une $ \mathcal{U}([0,1]) $ 
\end{exmp}

\begin{exmp}[Autre exemple]
    $ X_1 = 1.67, X_2= 1.3, X_3=0.01, X_4=2.48, X_5=0.11 $ Est-ce que les $ X_i \sim \mathcal{E}(1) $ ? On applique le test de Kolmogorov-Smirnov. 
    \begin{table}[!ht]
        \centering
        \begin{tabular}{|l|l|l|l|l|l|}
        \hline
            $X_{(i)}$ & 0.01 & 0.11 & 1.3 & 1.67 & 2.48 \\ \hline
            $F_n$ & 0.2  + 1/n & 0.4 & 0.6 & 0.8 & 1 \\ \hline
            $F(t) = 1 - e^{-x}$ & 0.01 & 0.1 & 0.72 & 0.81 & 0.91 \\ \hline
            Après le saut :$\left| \frac{i}{n} - F(X_{(i)}) \right|$ & 0.19 & 0.3 & 0.12 & 0.01 & 0.09 \\ \hline
            Avant le saut : $\left| \frac{i-1}{n}- F(X_{(i)}) \right|$ & 0.01 & 0.1 & \textbf{0.32} & 0.21 & 0.11 \\ \hline
        \end{tabular}
        \label{tab2}
    \end{table}
    \[
        h_{F_5,F} = 0.32
    .\]
    Test de niveau 99\% : Rejet si $ h \leq 0.6689 $ comme $ 0.32 \leq 0.6685 $ on conserve $ H_0 $ 
\end{exmp}

\underline{Nouveau cours du 27/01} \\

\textbf{Rappel du cours précédent}

On a vu le test de Kolmogorov-Smirnov : $ X_1, \dots, X_n $ iid. de fdr. $ F_{X_1} $. \\
Fonction de répartion cible $ F $ 
\[
    H_0 = F_{X_1} = G \text{ contre } H_1 = F_{X_1} \neq F
.\]
On calcule $ h(F_n, F) = \sup _{t \in \mathbb{R}} \left| F_n(t) - F(t) \right| $. \\
La loi de $ h(F_n, F) $ est tabimée, il suffit alors pour un niveau $ \alpha  $ donnée de vérifier si 
\[
    h(F_n, F) > S_\alpha \text{ le seuil au niveau } \alpha 
.\]

\textbf{Début du cours}

Si $ n $ est grand, on ne dispose pas de la table de $ h(F_n,F) $. Solution : Utiliser un test asymptotique.

\begin{thm}[]
    Soit $ h_n = \sup _{t \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{U_i \leq t} - t \right| $ à $ U_1, \dots, U_n $ sont des va. iid. de loi uniforme sur $ [0,1] $ 
    \[
        \sqrt[]{n}h_n \to ^{\mathcal{L}}_{n \to \infty } W_{\infty }
    .\]
    où $ P(W_\infty \leq t) = 1 - 2 \sum_{h=1}^{+\infty}(-1)^{k+1} e^{-2k^2t^2} $. 

    Bonne nouvelle : La loi de $ W_\infty  $ est tabulée !! 
\end{thm}
\begin{exmp}[Théorique de l'utilisation]
    Si $ n \geq 30 $. Pour avoir $ S_\alpha  $ tel que $ P(h_n > S_\alpha ) \approx 1 - \alpha $. \\
    Si je prends $ k_\alpha  $ tel que $ P(W_\infty > k_\alpha ) = 1 - \alpha  $ ($ k_\alpha  $ est le quantile d'ordre $ 1-\alpha  $ de $ W_\infty  $ ). \\
    Alors, si on pose $ S_\alpha = \frac{k_\alpha }{\sqrt[]{n}} $ on a : 
    \[
        P(h_n \geq S_\alpha ) = P(h_n \geq \frac{k_\alpha }{\sqrt[]{n}}) = P(\sqrt[]{n} h_n > k_\alpha ) \approx P(W_\infty \geq h_\alpha )
    .\]
    Conclusion : Si $ n $ est grand (pas dans la table), on prend $ s_\alpha  = \frac{k_\alpha }{\sqrt[]{n}}$ à $ h_\alpha  $ est le quantile d'ordre $ 1-\alpha  $ de $ W_\infty  $ 
\end{exmp}

\paragraph{Qu'est ce que $ W_\infty $ }

\begin{gather*}
    \sqrt{n} h_n = \sqrt{n}  \sup _{t \in \mathbb{R}}  \left| \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}_{U_i \leq t} - F(\mathbbm{1}_{U_i \leq t}) \right| \\
        = \sup _{t \in \mathbb{R}} \left| \sqrt{n} (\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{U_i \leq t}- F(\mathbbm{1}_{U_i \leq t}) \right| 
\end{gather*}
Cette quantité est approximativement une $ \mathcal{N}(0, t(1-t)) $ 
\[
    Gt\to \sqrt{n} (\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{U_i \leq t}- F(\mathbbm{1}_{U_i \leq t})
.\]
Le graphe de $ G $ est aléatoire et est disponible sur moodle (ça resemble à un cours de la bourse, dans notre cas on appelle ça un pont Brownien).

Pour la culture : un inégalité bien pratique 
\begin{thm}[Inégalité DKW]
    Inégalité de Dvoretsky-Kiefer-Wolfanitz : $ X_i $ va. iid.
    \[
        \forall n \in \mathbb{N}, \forall \epsilon > 0, \mathbb{P}(\sup _{t \in \mathbb{R}} \left| F_n(t) - F_{X_1}(t) \right| > \epsilon ) \leq 2 e^{-2n \epsilon ^2}
    .\]
    Cette inégalité est \begin{itemize}
        \item Non asymptotique
        \item Pas génial si $ n $ petit
    \end{itemize}
    Mais elle permet aussi de construire une zone de rejet.
\end{thm}

\paragraph{Kolmogorov-Smirnov en pratique}
On fait ce test si 
\begin{enumerate}
    \item Les $ X_i $ semblent provenir d'une loi à fonction de répartition continue. $ \Rightarrow  $ on n'a pas plusieurs fois la même valeur (sauf si celle-ci on était arrondi).\\
    Par exemple : si on voit 14 fois la même valeur $\rightarrow$ on utilise pas KS. Mais si on voit 2 fois la même valeur $\rightarrow$ c'est jouable
    \item Fonctionne $ \forall n $ : même si $ n $ est petit, ce test est pertinent (alors qu'un test du khi-deux qu'on verra plus tard est exclusivement asymptotique)
    \item Si $ n \geq 100 $, on fait le test asymptotique. Sinon on peut faire un test non asymptotique.
\end{enumerate}

\subsection{Ajustement à une famille de lois}
On veut savoir si nos observations iid. proviennent d'une certaine famille de lois. 
\begin{exmp}[]
    \begin{itemize}
        \item Est-ce que la loi $ X_i $ sont des $ \mathcal{E}(\lambda) $ pour $ \lambda >0 $ ?
        \item Est-ce que la loi $ X_i $ sont des $ \mathcal{N}(m, \sigma ^2) $ pour $ m \in \mathbb{R, \sigma ^2 > 0} $ ?
        \item Est-ce que la loi $ X_i $ sont des $ \mathcal{B}(n,p) $ pour $ m \in \mathbb{N}, p \in [0,1] $ ?
    \end{itemize}
\end{exmp}
Malheureusement, il est impossible de répondre à cette question en toute généralité. 

Cependant il y a deux exemple important qu'on peut traiter. 

\paragraph{Adéquation à une famille d'exponentielle}
Données : $ X_1, \dots, X_n $ iid. loi inconnue
\begin{itemize}
    \item $ H_0: $ les $ X_i $ sont $ \mathcal{E}(\lambda ) $ pour un certain $ \lambda \in \mathbb{R}^+_* $ 
    \item $ H_1: $ les $ X_i $ ne sont pas exponentiels.
\end{itemize}
Idée : On utilise $ h(F_n, F_\lambda ) $ pour un $ F_\lambda  $ bien choisis:
\[
    F_\lambda = (1-e^{-\lambda x})\mathbbm{1}_{x>0}
.\]
Si on veut tester $ X_i \sim \mathcal{E}(\lambda ) $, $ \lambda  $ fixée, on regarde 
\[
    h(F_n, F_\lambda ) = \sup _{t \in \mathbb{R}} \left| F_n(t) - (1 - e^{-\lambda t}) \mathbbm{1}_{t>0} \right| 
.\]
Problème : $ \lambda$ est inconnu $\Rightarrow  $ On l'estime !
\[
    \overline{\lambda }_n = \frac{n}{\sum_{i=1}^{n}X_i} \text{ estimateur Maximum Vraisemblance de }\lambda 
.\]
On regarde : $ X_i $ iid $ \mathcal{E}(\lambda ) $ 
\[
    h(F_n, F_{\overline{\lambda }_n }) = \sup _{t \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{X_i \leq t} - (1-e^{-\overline{\lambda}_n x})\mathbbm{1}_{t>0} \right| 
.\]
Miracle : La loi de $ h(F_n, F_{\overline{\lambda }_n}) $ ne dépend pas de $ \lambda  $, mais uniquement de $ n $. 

Si les $ (Y_i)_{i \in \mathbb{N}} $ sont iid. de loi $ \mathcal{E}(1) $, les $ (\frac{1}{\lambda } Y_i)_{i \in \mathbb{N}} $ sont iid de loi $ \mathcal{E}(\lambda ) $. Pour comprendre la loi de $ h(F_n, F_{\overline{\lambda }_n}) $, je peux remplacer les $ X_i $ par $ \frac{1}{\lambda }Y_i $.
\begin{align*}
    h(F_n, F_{\overline{\lambda }_n}) &= ^{loi} \sup _{t \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{\frac{Y_i}{\lambda } \leq t} - (1 - e^{- \frac{n}{\sum_{i=1}^{n} Y_i /\lambda } t }\mathbbm{1}_{t > 0}) \right|  \\
        &= \sup _{t \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{Y_i \leq  \lambda t} - (1 - e^{- \frac{n}{\sum_{i=1}^{n}Y_i} \lambda t }\mathbbm{1}_{\lambda  t > 0}) \right| \text{ or } \mathbbm{1}_{t>0} = \mathbbm{1}_{\lambda t} \\
        &= \sup _{s \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{Y_i \leq s} - (1-e^{- \frac{n}{\sum_{i=1}^{n}Y_i} s})\mathbbm{1}_{s>0} \right| \text{ avec } s=\lambda t
\end{align*}
Cela ne dépend pas de $ \lambda  $ mais seulement de $ n $. On peut tabuler ! (Malheureusement elle n'a pas de nom) et construire un test de KS.

\paragraph{Adéquation à une loi normale}
On peut adapter le test précédent pour des gaussiennes en estimant $ m $ et $ \sigma ^2 $ avec $ \overline{X}_n = \frac{1}{n}\sum_{i=1}^{n}X_i $ et $ V_n = \frac{1}{n-1} \sum_{i=1}^{n}(X_i - \overline{X}_n)^2 $ et construire un test. \\
Cela s'appelle le test de normalité de \textbf{Lilliefors} (voir exo de TD pour la suite)

\subsection{Le test du $ \mathcal{X}^2 $ d'ajustement}
La lettre grecque $ \mathcal{X} $ se prononce "khi". 

On dispose de $ X_1, \dots, X_n $ va. iid. \\
On se place dans le cas particulier où les $ X_i $ sont à valeurs dans un ensemble fini $ \{x_1, \dots, x_d\} $. La loi des $ X_i $ est donc entièrement déterminée par la donnée de $ p_k = P(X_1 = x_k) $ pour tout $ k $. Le vecteur $ p=(p_1, \dots, p_d) $ caractérise la loi des $ X_i $ 
\begin{rem}[]
    On sait que $ p_1 + \dots + p_d = 1 $
\end{rem}
Hypothèse : $ \forall h \in \{1,\dots,d\}, p_k > 0$. On ne s'est pas trompés dans le support, il faut prendre le plus petit $ d $. \\
Ces restrictions ne sont pas si contraignantes dans beaucoup de cas pratiques, elles sont automatiquement vérifiées

\begin{exmp}[]
    \begin{itemize}
        \item Réponse à un questionnaire QCM : la réponse prend un nombre fini de valeurs
        \item Une notes sur 20 d'un examen
        \item Des variables qualitatives : fille/garçons, couleur des yeux
    \end{itemize}
\end{exmp}

On a des observations $ X_1, \dots, X_n $ de loi inconnue $ p=(p_1, \dots, p_d) $. On veut savoir si $ p=p^{ref} $ pour un vecteur $ p^{ref} $ fixé. 
\begin{align*}
    H_0 &= p = p^{ref} \text{ i.e. } \forall k \in \{1,\dots,d\}, p_k = p_k^{ref} \\
    H_1 &= p \neq p^{ref} \text{ i.e. } \exists k \in \{1, \dots, d\}: p_k \neq p_k^{ref}
\end{align*}

\paragraph{Préparatifs, introduction } Si on trie nos valeurs 
\begin{table}[!h]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
        ~ & x1 & x2 & ... & xs \\ \hline
        Nombre d'observation & 17 & 23 & ... & 12 \\ \hline
    \end{tabular}
\end{table}
$ p^{ref} = (0;3, 0.1, 0.2, 0.2, 0.2) $ On a envie de regarder $ \overline{p}_1 = \frac{\text{Nombre de } n_1}{n}, \dots, \overline{p}_s = \frac{\text{Nombre de }n_s}{n}$. On a envie de construire quelque chose avec ces estimateurs 

\paragraph*{Notation} 

\[
    \forall k \in \{1,\dots,d\}, N_{k,n} = \sum_{i=1}^{n}\mathbbm{1}_{X_i = x_k}
.\]
Les $ N_{k,n} $ sont les effectifs observés.
\[
    \forall k \in \{1,\dots,d\} \overline{p}_{k,n} = \frac{N_{k,n}}{n} = \frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{X_i = x_k}
.\]
Les $ \overline{p}_{k,n} $ sont les proportions observés. On note $ \overline{p}_n = (\overline{p}_{1,n}, \dots, \overline{p}_{d,n}) $ \\
Sous $ H_0 $, les $ \overline{p}_{k,n} $ devraient être proches des $ p_k^{ref} $ 

\begin{note}[]
    Comme dans KS, on vas trouver une formule reliant les deux et pouvant être tabuler pour faire des tests. Mais elle est pas vraiment démontrable à notre niveau et utilise des vecteurs gaussiens
\end{note}

\begin{thm}[]
    Sous $ H_0 $ on note 
    \begin{align*}
        D(\overline{p}_n, p^{ref}) &= n \sum_{k=1}^{d}\frac{(\overline{p}_{k,n} - p_k^{ref})^2}{p_k^{ref}} \\
        D(\overline{p}_n, p^{ref}) &\to ^{\mathcal{L}}_{n \to \infty } \mathcal{X}^2(d-1)
    \end{align*}
    Sous $ H_1 $ 
    \[
        D(\overline{p}_n, p^{ref}) \to^{p.s}_{n \to \infty } + \infty 
    .\]
\end{thm}
\begin{rem}[Autre formulation, qu'on utilise en TD !]
    On peut aussi écrire 
    \begin{align*}
        D(\overline{p}_n, p^{ref}) = \sum_{k=1}^{d} \frac{(N_{k,n} - np_k^{ref})^2}{n p_k^{ref}}
    \end{align*}
    Si on note $ N_k^{ref} = np_k^{ref} $ l'effectifs attendu, alors cela devient 
    \[
        D(\overline{p}_n, p^{ref}) = \sum_{i=1}^{d} \frac{(N_{k,n} - N_k^{ref})^2}{N_k^{ref}}
    .\]
    $ N_k^{ref} $ n'est pas un entier en général
\end{rem}

\paragraph{Le test du $ \mathcal{X}^2 $} 
\begin{itemize}
    \item Données : $ X_1, \dots, X_n $ à valeur dans $ \{x_1, \dots, x_d\} $ 
    \item $p^{ref} $ qu'on veut tester 
    \item Niveau $ \alpha  $ 
\end{itemize}
Soit $ h_\alpha  $ le quantile d'ordre $ 1-\alpha  $ de la loi $ \mathcal{X}^2 (d-1) $ alors 
\begin{itemize}
    \item Si $ D(\overline{p}_n, p^{ref}) \geq h_\alpha  $ on rejette $ H_0 $ 
    \item Sinon $  D(\overline{p}_n, p^{ref}) < h_\alpha $ on conserve $ H_0 $ 
\end{itemize}
\textbf{Attention:} Ce test est uniquement asymptotique ! 

Condition d'utilisation : 
\[
    \forall k \in \{1, \dots, d\}, np_k^{ref}(1-p_k^{ref}) \geq 5
.\]
Cela implique $ n \geq 20 $ mais en général il faut beaucoup plus 

\begin{exmp}[dé truqué]
    On dispose d'un dé douteux, on releve les résultats de 100 lancés et on veut determiner si il est pipé ou non.
    \begin{table}[!h]
        \centering
        \begin{tabular}{|l|l|l|l|l|l|l|}
        \hline
            ~ & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
            Effectifs & 16 & 20 & 19 & 10 & 17 & 18 \\ \hline
            Proportions & 0.16 & 0.2 & 0.19 & 0.1 & 0.17 & 0.18 \\ \hline
        \end{tabular}
    \end{table}

    \textbf{Condition} : $ 100*\frac{1}{6}*\frac{5}{6} = \frac{500}{36} = 13.88 > 5 $ c'est bon le test du $ \mathcal{X}^2 $ est applicable.
    \begin{itemize}
        \item $ H_0: $ dè non truqué $ \Leftrightarrow p^{ref} =  (\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},) $
        \item $ H_1: $  dé truqué $ p \neq p^{ref} $ 
    \end{itemize}
    On calcule 
    \begin{align*}
        D &= 100 * [\frac{(0.16 - \frac{1}{6})^2}{1/6} + \frac{(0.2 - \frac{1}{6})^2}{1/6} + \dots + \frac{(0.18 - \frac{1}{6})^2}{1/6}] \\
            &= 600 \sum_{k=1}^{6}(\overline{p}_k - \frac{1}{6})^2 = 3.8
    \end{align*}
    Pour faire un test à 90\%, on doit comparer cette valeur avec le quantile d'ordre d'une loi $ \mathcal{X}^2 (6-1) $ degrés de liberté. Lecture de table : $ k= 9.24 $.\\
    Ainsi comme $ D=3.28 < 9.24 $, on conserve $ H_0 $ le dé est équilibré
\end{exmp}

\underline{Nouveau cours du 03/02} \\

\textbf{Bilan jusqu'à présent} \\
Le test du $ \mathcal{X}^2 $ "basique" permet de tester l'adéquation de données iid $ X_1, \dots, X_n $ à valeurs dans $ \{x_1, \dots, x_d\} $ à une loi discrète sur $\{x_1, \dots, x_d\}$ caractérisé par un vecteur de probabilité : $ p=(p_1, \dots, p_d) $ 

\paragraph{Mise en place concrette :} 
\begin{enumerate}
    \item Etape 0 : On vérifie les conditions 
    \[
        \forall k \in \{1, \dots, d\}, n*p_k \geq 5
    .\]
    C'est la condition de Cochran (1954), il avait testé cas possible en observant l'approximation faites.
    \item Etape 1 : On calcule les effectifs et proportions observées : $ N_{k,n} $ et $ \hat{p}_{k,n} $  
    \item Etape 2 : Calcul de la statistique de test 
    \[
        D = n \sum_{d}^{k=1} \frac{(\hat{p}_{k,n} - p_k)^2}{p_k}
    .\]
    \item Etape 3 : Détermination de la zone de rejet au niveau $ \alpha  $. On lit $ h_\alpha  $ le quantile d'ordre $ 1-\alpha  $ de la loi $ \mathcal{X}^2(d_1) $ 
    \item Etape 4 : Décisions \begin{itemize}
        \item si $ D > h_\alpha  $, on rejette $ H_0 $ (au niveau $ \alpha  $ ). 
        \item Si $ D \leq h_\alpha  $ on conserve $ H_0 $ 
    \end{itemize}
\end{enumerate}

\paragraph{Test du $ \mathcal{X}^2 $ avec fusion des classes} 
Que fait-on si la condition $ np_k \geq 5 $ n'est pas vérifiée ? On fusionne des classes ! 

\begin{exmp}[]
    On a observé des réponses à un questionnaire. On veut tester l'adéquation à la loi $ p=(\frac{1}{4}, \frac{1}{4}, \frac{7}{16}, \frac{1}{16}) $ avec $ n=40 $

    \begin{table}[!h]
        \centering
        \begin{tabular}{|l|l|l|l|l|}
        \hline
            Modalité & 1 & 2 & 3 & 4 \\ \hline
            Effectif & 10 & 18 & 11 & 1 \\ \hline
        \end{tabular}
    \end{table}

    Vérification des conditions du test du $ \mathcal{X}^2 $ 
    \begin{align*}
        40*p_1 = \frac{40}{4} = 10 > 5
        40*p_2 = \frac{40}{4} = 10 > 5
        40*p_3 = \frac{40*7}{16} = 10 \geq  5
        40*p_4 = \frac{40}{16} = 10 < 5 \text{ condition non vérifiée !}
    \end{align*}
    On fusionne des colonnes de manière à remplir les conditions. On fusionne les colonnes 3 et 4 pas exemple.

    \begin{table}[!ht]
        \centering
        \begin{tabular}{|l|l|l|l|}
        \hline
            Modalité & 1 & 2 & 3 ou 4 \\ \hline
            Effectif & 10 & 18 & 12 \\ \hline
        \end{tabular}
    \end{table}

    La nouvelle probabilité de référence devient 
    \[
        p^{ref}_{nouvelle} = ( \frac{1}{4}, \frac{1}{4}, \frac{7}{16} + \frac{1}{16}) = (\frac{1}{4}, \frac{1}{4}, \frac{1}{2})
    .\]
    Nouvelle condition : 
    \begin{align*}
        40 * p_1 = 10 > 5
        40 * p_2 = 10 > 5
        40 * p_3 = \frac{40}{2} = 20 > 5
    \end{align*}
    Si on applique le test du $ \mathcal{X}^2 $ "de base", on obtient un test asymptotique de niveau $ \alpha $ pour le cas à 3 classes (fait avec un $ \mathcal{X}^2(2) $ ), donc c'est aussi un test asymptotique de niveau $ \alpha  $ pour le cas à 4 classes.
    
    \begin{rem}[]
        Si on prend $ q = (\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}) $ qui appartient à $ H_1^4 $ car $ q \neq p = (\frac{1}{4}, \frac{1}{4}, \frac{7}{16}, \frac{1}{16}) $. En fusionnant ce cas particulier, on se retrouve dans $ H_0^3 $.
        \[
            q \in H_1^4 \to q^{reduit} = (\frac{1}{4}, \frac{1}{4}, \frac{1}{2}) \in H_0^3
        .\]
        On perd donc en information quand on fusionne des colonnes. La puissance du test se réduit car on se retrouve avec des cas dans $ H_1 $ et dans $ H_0 $.\\
        L'opération de fusion des colonnes permet toujours de construire un test de niveau asymptotique $ \alpha  $ au détriment de la puissance.
    \end{rem}    
\end{exmp}

\subsection{Le test du $ \mathcal{X}^2 $ pour une loi discrète}
Données : $ X_1, \dots, X_n $ observation iid. \\
Loi cible à valeur dans $ \mathbb{N} $ caractérisée par 
\[
    p = (p_k)_{k \in \mathbb{N}}
.\]
Exemple pour une poisson 
\[
    p_k = \frac{\lambda^k e^{-\lambda }}{k!}
.\]
Est-ce que la loi des $ X_i $ est donnée par $ p $ ?  C'est à dire 
\[
    \forall k \in \mathbb{N}, P(X_1 = k) = p_k \text{ ?}
\]

\begin{exmp}[]
    \begin{table}[!ht]
        \centering
        \begin{tabular}{|l|l|l|l|l|l|l|l|l|}
        \hline
            Valeur & 0 & 1 & 2 & 3 & 4 & 5 & 6 & ... \\ \hline
            Effectif & 5 & 8 & 12 & 7 & 2 & 1 & 0 & ... \\ \hline
        \end{tabular}
    \end{table}
    "On ne peut pas faire un $ \mathcal{X}^2 $ avec une infinité de degrés de liberté" $\rightarrow$ On regroupe les classes à partir d'un certain rang
    \begin{table}[!ht]
        \centering
        \begin{tabular}{|l|l|l|l|l|l|}
        \hline
            Valeur & 0 & 1 & 2 & 3 & 4 et plus \\ \hline
            Effectif & 5 & 8 & 12 & 7 & 4 \\ \hline
        \end{tabular}
    \end{table}
    On voudrait regarder $ np_0, np_1, np_2, np_3 $ et pour la 4ème classe $ n(\sum_{k=4}^{+ \infty } p_k) $. Les classes sont déterminées afin que toutes les conditions soient satisfaites. 

    En pratique, on regarde à partir de quel indice la condition $ np_k < 5 $ ne fonctionne plus, puis on regroupe à partir de la 
\end{exmp}

\paragraph{En pratique}
    Donnée : $ X_1, \dots, X_n$ \\
    Loi cible : $p = (p_k)_{k \in \mathbb{N}^*}$
    \begin{enumerate}
        \item Etape 0 : On détermine les classes en calculant $ np_1, np_2, ... $ et ainsi de suite.
        \item On regroupe les classes qui ne vérifient pas la condition
        \item On calcule les effectifs de chaque classes $ N_{1,n}, \dots, N_{c-1, n}, N_{c,n} $ avec $ c $ l'effectif dans la classe agglomérée
        \item On calcule les proportions observées $ \hat{p}_{k,n} $ et la stat de test $ D = n \sum_{k=1}^{c} \frac{(\hat{p}_{k,n} - p_k)^2}{p_k^\prime } $ où $ p_k^\prime = p_k $ si $ k \leq c_1 $ et $ p^\prime _c = \sum_{k=c}^{+\infty } p_l $ 
        \item On détermine la zone de rejet à l'aide du quantile d'ordre $ 1 - \alpha $ d'une loi $ \mathcal{X}^2(c-1) $, noté $ h_\alpha  $ Et on décide de conserver $ H_0 $ si $ D \leq h_\alpha  $, on rejette sinon.
    \end{enumerate}

\paragraph{Limite}
Ce test permet de tester l'adéquation à n'importe quelle loi discrète au niveau $ \alpha  $. Cependant, dès lors qu'on regroupe des classes (ce qui est obligatoire ici) on perd la consistance du test. 


\subsection{Le test du $ \mathcal{X}^2$ pour une loi continue}
Données: $ X_1, \dots, X_n $ iid. \\
Loi cible : $ L $ la loi d'une v.a. $ L $ (par exemple de densité $ g $) \\
Idée : Transformer les données en les regroupant par paquets. 

Soient $ I_1, \dots, I_d $ des intervalles qui forment une partition du support de $ L $. (disjoints, dont l'union couvre toutes les valeurs de $ L $) Voir \ref{fig4} \\
Condition 
\[
    \forall k \in \{1, \dots,d \} n * P(L \in I_k) \geq 5
.\]
\begin{figure}[!htbp]
    \centering
    \includegraphics*[width=.75\textwidth]{fig4.png}
    \caption{Illustration de la partition de L}
    \label{fig4}
\end{figure}
On crée de nouvelle variables $ Y_i $ : Numéro de l'intervalle dans lequel est $ X_i $ 
\[
    P(Y_1=k) = P(X_i \in I_k) = p_k (\text{sous }H_0)
.\]
On a alors : $ Y_1, \dots, Y_n $ variables à valeur dans $ \{1,...,d\} $, avec comme proba cible : $ p=(p_i = P(L \in I_i), \dots, p_d = P(L \in I_d)) $. 

On applique alors le test du $ \mathcal{X}^2 $ "basique" aux variable $ Y_i $. Cela fournit un test asymptotique de niveau $ \alpha  $. Le tableau à considérer est : 
\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
        Intervale & $I_1$ & $I_2$ & $I_3$ & ... & $I_d$ \\ \hline
        Effectif & . & . & . & ... & . \\ \hline
    \end{tabular}
\end{table}

\paragraph{Bilan de la méthode}
Aspects positifs : 
\begin{itemize}
    \item \textbf{Fonctionne pour toutes les lois}
    \item Facile à faire
\end{itemize}

Aspects négatifs : 
\begin{itemize}
    \item Problème de consistance. Regrouper les variables par intervalle ruiner l'erreur de seconde espèce.
    \item Asymptotique
    \item Dépendant du choix des intervalles. Ce qui n'est pas canonique.
\end{itemize}


\subsection{Le $ \mathcal{X}^2 $ d'ajustement à une famille paramétrique de loi}
On dispose d'observation iid. $ X_1, \dots, X_n $. \\ 
On veut savoir si la loi des $ X_i $ fait partie d'une famille paramétrique $ \mathcal{F} =  (P_\theta )_{\theta \in \Theta} $ à $ \Theta \subset \mathbb{R}^M $.\\
Par exemple \begin{itemize}
    \item Lois de Poisson $ (\mathcal{P}ois(\lambda ))_{\lambda \in \mathbb{R}^+_*}, M=1 $ 
    \item Lois Exponentielles : $ (\mathcal{E}(\lambda ))_{\lambda \in \mathbb{R}^+_*}, M=1 $
    \item Lois géométrique : $ (\mathcal{G}eom(p))_{p \in ]0,1[}, M=1 $
    \item Lois normales : $ (\mathcal{N}(m, \sigma ^2))_{m \in \mathbb{R}, \sigma ^2 \in \mathbb{R}^+_*}, M=2 $
\end{itemize}
Les hypothèses :
\begin{itemize}
    \item $ H_0 = $ la loi des $ X_i $ appartient à $ \mathcal{F} $ 
    \item $ H_1 = $ la loi des $ X_i $ n'appartient pas à $ \mathcal{F} $ 
\end{itemize}
\begin{enumerate}
    \item Etape 1 : Soit $ \hat{\theta }_n $ l'estimateur du maximum de vraisemblance de $ \theta  $ (pour $ P_\theta  $ ). On estime \textbf{tous} les paramètres de la loi $ (p_1^{\hat{\theta }_n}, \dots, p_d^{\hat{\theta }_n}) $ 
    \item Etape 2 : On vas tester l'ajustement de $ X_1, \dots, X_n $ à $ P_{\hat{\theta }_n} $ On calcule les fréquences observées $ \hat{p}_{k,n} $.
\end{enumerate}
\textbf{Erreur à ne pas commettre :} il est faut de dire que 
\[
    D = n \sum_{k=1}^{d}\frac{(\hat{p}_{k,n} - p_k^{\hat{\theta }_n)^2}}{p_k^{\hat{\theta }_n}} \to \mathcal{X}^2(d-1)
.\]
\begin{thm}[]
    Sous $ H_0 $ , 
    \[
        D = n \sum_{k=1}^{d}\frac{(\hat{p}_{k,n} - p_k^{\hat{\theta}_n})^2}{p_k^{\hat{\theta }_n}} \to \mathcal{X}^2 (d-1-M)
    .\]
    Avec \begin{itemize}
        \item $ d= $ Nombre de classes à la fin, après regroupement éventuel
        \item $ M =  $ nombre de paramètre
    \end{itemize}
\end{thm}

\paragraph{En pratique}
\begin{enumerate}
    \item Etape 1 : Soit $ \hat{\theta }_n $ l'estimateur du maximum de vraisemblance de $ \theta  $ (pour $ P_\theta  $ ). On estime \textbf{tous} les paramètres de la loi $ (p_1^{\hat{\theta }_n}, \dots, p_d^{\hat{\theta }_n}) $ 
    \item Etape 2 : On vas tester l'ajustement de $ X_1, \dots, X_n $ à $ P_{\hat{\theta }_n} $ On calcule les fréquences observées $ \hat{p}_{k,n} $.
    \item Etape 3 : Vérification des conditions $ np_k^{\hat{\theta }_n} $ et possible regroupement en classes 
    \item Etape 4 : Calcul de la stat de test $ D $ 
    \item Etape 5 : Zone de rejet : lecture de $ H_\alpha  $ le quantile d'ordre $ 1-\alpha  $ d'une $ \mathcal{X}^2(d-1-M) $ 
    \item Etape 6 : Décision 
        \begin{itemize}
            \item $ D > h_\alpha  $ on rejette $ H_0 $ 
            \item $ D \leq h_\alpha  $ on conserve $ H_0 $ 
        \end{itemize}
\end{enumerate}

\underline{Nouveau cours du 10/02} \\

\begin{exmp}[Test d'ajustement à un loi de Poisson]
    On dispose d'observation $ X_1, \dots, X_n $ iid. (représentant le nombre d'heure entre 2 pannes de métro). On veut tester pour savoir si les données proviennent d'une loi de Poisson. \\
    \begin{rem}[Rappel]
        $ Z \sim Pois(\lambda), P(Z=k) = \frac{\lambda^k e^{-\lambda}}{k!}, \lambda \in \mathbb{R}^* $ \begin{itemize}
            \item $ H_0 $ La loi des observation est $ Pois(\lambda ) $ pour un certain $ \lambda > 0 $ 
            \item $ H_1 $ la loi des $ X_i $ n'est pas une loi de Poisson
        \end{itemize}
    \end{rem}
    \begin{enumerate}
        \item Estimer les paramètres (par un maximum de vraisemblance) : \\
            On rappelle (1er semestre) que l'EMV pour $ \lambda  $ est 
            \[
                \bar{\lambda }_n = \frac{1}{n}\sum_{i=1}^{n}X_i
            .\]
            Données : 
            \begin{table}[!h]
                \centering
                \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
                \hline
                    Valeurs & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & ... \\ \hline
                    Effectif & 14 & 22 & 20 & 25 & 7 & 9 & 2 & 1 & 0 & 0 & 0 \\ \hline
                \end{tabular}
            \end{table}
            Sur ces 100 données, on calcule $ \bar{\lambda } _n $ : 
            \[
                \bar{\lambda } = \frac{1}{100}(14*0 + 22*1 + \dots + 1*7) = 2.29
            .\]
        \item Calcul de la statistique de test comme si on faisait un $ \mathcal{X}^2 $ d'ajustement à une $ \mathcal{P}(2.29) $. \\
            Si $ Z \sim \mathcal{P}(2.29), P(X=k) = (2.29)^k \frac{e^{-2.29}}{k!} = p_k $. On calcule les $ np_k $ 
            \begin{table}[!h]
                \centering
                \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
                \hline
                    k & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & ... \\ \hline
                    $100p_k$ & 10.13 & 23.19 & 26.55 & 20.27 & 11.6 & 5.3 & 2.03 & 0.66 & ... \\ \hline
                \end{tabular}
            \end{table}
            On détermine les classes à regrouper pour avoir $ np_k \geq 5 $. On se rend compte rapidement qu'il faut regrouper $ 5 $ à $ +\infty $
            \begin{table}[!h]
                \centering
                \begin{tabular}{|l|l|l|l|l|l|l|}
                \hline
                    k & 0 & 1 & 2 & 3 & 4 & 5 et + \\ \hline
                    $100p_k$ & 10.13 & 23.19 & 26.55 & 20.27 & 11.6 & $100 - \sum autres = 8.26$ \\ \hline
                \end{tabular}
            \end{table}
            Calcul de la statistique de test : 
            \[
                D = 100 \sum_{k=0}^{5}\frac{(p_{k,n} - p_k)^2}{p_k} = \sum_{k=0}^{5}\frac{(N_{k,n} - 100 p_k)^2}{100 p_k} = 7.78
            .\]
        \item Zone de rejet au niveau $ \alpha = 5\% $ \\
            On li dans la table le quantile d'ordre $ 1 - \alpha = 0.95 $ de la loi $ \mathcal{X}^2(6 - \text{ Nombre de classe } - \text{ Nombre de paramètre estimé }) = \mathcal{X}^2(6-1-1) $. Ici $ k_{0.95} = 9.48$. \\
            CCL : Comme $ D = 7.78 \leq k_{0.95} = 9.48 $ on conserve $ H_0 $.
    \end{enumerate}
\end{exmp}

\begin{rem}[Chapitre 1]
    \begin{itemize}
        \item Remarque sur le $ \mathcal{X}^2 $ d'ajustement à une formule paramétrique de lois : \\
            Principe : \begin{enumerate}
                \item On estime
                \item On calcule comme si on faisait un $ \mathcal{X}^2 $ d'ajustement à une seule loi
                \item Attention au degrés de liberté dans la zone de rejet ! 
            \end{enumerate}
            \item Remarque sur la consistance : Si le nombre de classes utilisées tend vers $ + \infty  $ quand $ n \to +\infty  $, le test du $ \mathcal{X}^2 $ est consistant.
    \end{itemize}
\end{rem}

\subsection{Bilan du chapitre}
On a deux test d'ajustement : Kolmogorov-Smirnov et $ \mathcal{X}^2 $ \begin{itemize}
    \item KS : ajustement à une loi de fdr. continue. Fonctionne pour toutes valeurs de $ n $. Si $ n $ grand, on prend $ \frac{1}{\sqrt[]{n}} $ quantile de $ W_\infty  $.\\
        \textbf{Attention : } Si $ n $ est grand sur des données réelles, KS est très sensible au bruit et rejette très souvent. Une erreur de $0.01$ sur la fdr. des données mène vite à un ... si $ n \geq 10^5 $.
    \item $ \mathcal{X}^2 $ : Test asymptotique, $ n \geq 50 $ au minimum + Condition. Fonctionne dans tous les cas.
\end{itemize}

\section{Loi de comparaison}
Dans ce chapitre, on dispose de deux jeux de données \begin{itemize}
    \item $ X_1, \dots, X_n $ avec $ n>0 $ iid.
    \item $ Y_1, \dots, Y_n $ avec $ n>0 $ iid.
\end{itemize}
On cherche à comparer les lois sous-jacentes. \begin{enumerate}
    \item Est-ce que les $ X_i $ et $ Y_j $ ont la même loi ? (homogénéité)
    \item Est-ce que les $ X_i $ sont indépendants des $ Y_j $ (indépendance)
    \item Est-ce que les loi de $ X_i $ et $ Y_j $ ont la même moyenne ou la même médiane ? 
\end{enumerate}
Deux cas de figure : 
\begin{itemize}
    \item Échantillon appariés : les $ X_i $ et $ Y_j $ proviennent d'une même mesure / tirage $ (X_i, Y_j) $. Cela implique $ n=m $. 
        \begin{exmp}[]
            On mesure la taille et le poids de pluviomètres à Roubaix et à Croix
        \end{exmp}
        
    \item Echantillons indépendants : si $ (X_1, \dots, X_n) $ est indépendant de $ (Y_1, \dots, Y_n) $ , on dira que les échantillons sont indépendants. 
\end{itemize}

\subsection{Le test d'homogénéité de Kolmogorov-Smirnov}
On dispose des données iid. $ (X_1, \dots, X_n) $ et $ (Y_1, \dots, Y_n) $. Les échantillons sont indépendants. On veut tester
\begin{itemize}
    \item $ H_0 $ : les $ X_i $ et $ Y_i $ ont la même loi, c'est à dire $ F_{X_1} = F_{V_1} $ où $ F_{X_1}, F_{Y_1} $ sont continues.
    \item $ H_1 $ les lois sont différentes
\end{itemize}
Comme pour le test d'ajustement de KS, on va construire un test non asymptotique se basant sur les fdr. empirique.\\
Notation : 
\begin{align*}
    F_n : &\mathbb{R} \to [0,1] & G_n: &\mathbb{R} \to [0,1] \\
        & t \mapsto \frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{X_i \leq t} & & t \mapsto \frac{1}{n}\sum_{j=1}^{n} \mathbbm{1}_{Y_j \leq t}
\end{align*}

\begin{thm}[].
    \begin{enumerate}
        \item Si $ F_{X_1} = F_{Y_1} $ alors la variable 
        \[
            h(F_n, G_n) = \sup _{t \in \mathbb{R}} \left| F_n(t) - G_n(t) \right| 
        .\]
        a même loi que la variable 
        \[
            h_{n,m} = \sup _{s \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{\mathbf{U_i} \leq s} - \frac{1}{n}\sum_{j=1}^{n} \mathbbm{1}_{\mathbf{V_i} \leq s} \right| 
        .\]
        avec $ (U_1, ..., U_n) $ et $ (V_1, \dots, V_n) $ sont deux échantillons indépendants de variable iid. uniformes sur $ [0,1] $.
        \item De plus si $ F_{X_1} \neq F_{Y_1} $ alors 
        \[
            h(F_n, G_n) \to _{n,m \to \infty } \left\| F_{X_1} - F_{V_1} \right\|_{\infty } = \sup _{t \in \mathbb{R}} \left| F_{X_1}(t) - F_{Y_1}(t) \right| > 0
        .\]
        \begin{proof}[Preuve: ]
            \begin{enumerate}
                \item D'après le théorème de simulation par inversion de la fdr. si $ U_1, \dots, U_n $ sont des variables aléatoire iid. uniforme sur $ [0,1], (F_{X_1}^{-1}(U_1), \dots, F_{X_1}^{-1}(U_n))$ a même loi que $ (X_1, \dots, X_n) $. \\
                Si $ U_1, \dots, U_n $ sont des variables aléatoire iid. uniforme sur $ [0,1], (F_{X_1}^{-1}(V_1), \dots, F_{X_1}^{-1}(V_n))$ a même loi que $ (Y_1, \dots, Y_n) $.

                Ainsi , $ h(F_n, G_n) $ a même loi que $ \sup _{s \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{F_{X_1}^{-1}(U_i) \leq t} - \frac{1}{n}\sum_{j=1}^{n} \mathbbm{1}_{F_{X_1}^{-1}(V_i) \leq t} \right|$ 

                Par les propriétés classique de l'inverse généralisée,
                \begin{align*}
                    F_{X_1}^{-1} (U_i) \leq t &\Leftrightarrow U_i \leq F_{X_1} (t) \\
                    F_{X_1}^{-1} (V_j) \leq t &\Leftrightarrow V_j \leq F_{X_1}(t) \\
                \end{align*}

                Ainsi $ A = \sup _{s \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{U_i \leq F_{X_1}(t)} - \frac{1}{n}\sum_{j=1}^{n} \mathbbm{1}_{V_i \leq F_{X_1}(t)} \right|  $ 
                \item Conséquence immédiate du théorème de Glivenko Cantelli.
            \end{enumerate}
        \end{proof}
    \end{enumerate}
\end{thm}

\subsubsection{Test d'homogénéité de Kolmogorov-Smirnov}
Données : $ (X_1, \dots, X_n) $ et $ (Y_1, \dots, Y_n) $ iid deux échantillon indépendants. $ H_0 : F_{X_1} = F_{Y_1} $ contre $ H_1 : F_{X_1} \neq F_{Y_1} $. \\ 
Statistique de test : on calcule 
\[
    \sup _{s \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{X_i \leq t} - \frac{1}{n}\sum_{j=1}^{n} \mathbbm{1}_{Y_j \leq t} \right| 
.\]
Zone de rejet au niveau $ \alpha  $ : \\
Soit $ k_\alpha  $ le quantile d'ordre $ 1 - \alpha  $ de la loi de 
\[
    \sup _{s \in \mathbb{R}} \left| \frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{U_i \leq s} - \frac{1}{n}\sum_{j=1}^{n} \mathbbm{1}_{V_j \leq s} \right| 
.\]
où $ (U_1, \dots, U_n) \bot (V_1, \dots, V_n) $ iid. $ \sim U([0,1]) $

CCL : Si $ h(F_n, G_n) \leq k_\alpha  $, on conserve $ H_0 $ au niveau $ \alpha  $. Sinon on rejette $ H_0 $ 

\begin{rem}[].
    \begin{enumerate}
        \item Ce test est de taille $ \alpha  $, si on utilise la table de $ h_{n,m} $.
        \item Si $ n $ et $ m $ sont trop grands, on utilise le résultat suivant : \\
            Sous $ H_0 $ 
            \[
                \sqrt[]{\frac{nm}{n+m}}h(F_n, G_n) \to ^{\alpha }_{n,m \to +\infty } W_\infty \text{ voir KS asymptotique}
            .\]
            On utilise alors comme zone de rejet $ \sqrt[]{\frac{n+m}{nm}}W_\infty  $ avec $ W_\infty  $ le quantile d'ordre $ 1 - \alpha  $ de $ W_\infty  $.

            \paragraph{En pratique (cas n et m grand)} : \\ 
            En R : $ X,Y $ vecteur, ks.test(X,Y) \\
            A la main : 
            \[
                F_n(t) - G_n(t) = \frac{\text{nb de } X_i \leq t}{n} - \frac{\text{nb de } Y_i \leq t}{m}
            .\]

            \begin{figure}[!htb]
                \centering
                \includegraphics*[width=0.5\textwidth]{fig5.jpg}
                \caption{<caption>}
                \label{<label>}
            \end{figure}
            
            On range par ordre croissant : 

            \begin{table}[!h]
                \centering
                \begin{tabular}{|l|l|l|l|l|l|l|}
                \hline
                    $ X_{(1)} $ & $ X_{(2)} $ & $ Y_{(1)} $ & $ X_{(3)} $ & $ Y_{(2)} $ & $ X_{(4)} $ & ... \\ \hline
                    $ \left| \frac{1}{n} - \frac{0}{m} \right|  $  & $ \left| \frac{2}{n} \right| $  & $ \left| \frac{2}{n} - \frac{1}{m} \right|  $  & $ \left| \frac{3}{n} - \frac{1}{m} \right|  $ & $ \left| \frac{3}{n} - \frac{2}{m} \right|  $ & $ \left| \frac{4}{n} - \frac{2}{m}\right|  $ & ... \\ \hline
                \end{tabular}
            \end{table}

            Je calcules les $ n+m $ quantités et je garde la plus grande valeur. \\
            Méthode inefficace : 
            \begin{table}[!h]
                \centering
                \begin{tabular}{|l|l|l|l|l|l|l|l|}
                \hline
                    $ X_i $  & $ X_{(1)} $ & $ X_{(2)} $ & ~ & $ X_{(3)} $ & ~ & $ X_{(4)} $ & ~ \\ \hline
                    $ Y_i $  & ~ & ~ & $ Y_{(1)} $ & ~ & $ Y_{(2)} $ & ~ & ~ \\ \hline
                    $ F_n $  & $ \frac{1}{n} $  & $ \frac{2}{n} $  & $ \frac{2}{n} $  & $ \frac{3}{n} $  & $ \frac{3}{n} $  & $ \frac{4}{n} $  & ... \\ \hline
                    $ G_m $  & 0 & 0 & $ \frac{1}{m} $  & $ \frac{1}{m} $  & $ \frac{2}{m} $  & $ \frac{2}{m} $  & ... \\ \hline
                \end{tabular}
            \end{table}

    \end{enumerate}
\end{rem}

\underline{Nouveau cours du 03/03} \\

\subsection{Les test du $ \mathcal{X}^2 $ d'indépendance et d'homogénéité}
\subsubsection{Le $ \mathcal{X}^2 $ d'indépendance}

\paragraph*{Rappel:} deux variables aléatoire réelle $ X, Y $ sont indépendante ssi 
\[
    \forall A,B \subset \mathcal{B}(\mathbb{R}), P(X \in A \text{ et } Y \in B) = P(X \in A)P(Y \in B)
.\]
De manière informelle, la connaissance de $ X $ ne donne aucune information sur $ Y $ 

\paragraph*{Données}: 

$ (X_1, Y_1), \dots, (X_T, Y_T)$ données appariés \textbf{iid.}.
Cela mène à deux échantillons iid. $ X_1, \dots, X_T  $ et $ Y_1, \dots, Y_T $. 

\textbf{Attention}: $ T = $ nombre totale de mesures

On veut déterminer si $ X_1 $ est indépendant de $ Y_1 \Leftrightarrow X_1 \perp Y_1$. \\
Ainsi on vas construire un test pour \begin{itemize}
    \item $ H_0 : X_1 \bot Y_1$ 
    \item $ H_0 : X_1 \not\bot Y_1$ ne sont pas indépendants
\end{itemize}
Quel genre de situation cela couvre-t-il ? 2 exemples : \begin{itemize}
    \item Apparition d'effets secondaire pour un traitement
    \item Effet d'un facteur : réussite au bac en fonction du sexe ? 
\end{itemize}

\textbf{Important:} Les données sont à valeurs dans un nombre \textbf{fini} de classes :\begin{itemize}
    \item $ X_1, \dots, X_T $ à valeurs dans $ A_1, \dots, A_M $ 
    \item $ Y_1, \dots, Y_T $ à valeurs dans $ B_1, \dots, B_N $ 
\end{itemize}
Si ce n'est pas le cas, on s'y ramène en \textbf{créant} des classes comme pour les autres test du $ \mathcal{X}^2 $ 

La loi de $ X_1 $ est caractérisé par 
\[
    p_m = P(X_1 \in A_m) \text{ pour }  m \in \{1, \dots, M\}
.\]
de même pour $ Y_1 $ 
\[
    q_n = P(Y_1 \in B_n) \text{ pour }  n \in \{1, \dots, N\}
.\]
Si on a accès à ces probabilités, \textbf{l'indépendance}se lit 
\[
    \forall m \in \{1, \dots, M\}, \forall n \in \{1, \dots, N\}
.\]
\[
    p_{m,n} = P(X_1 \in A_n \text{ et } Y_1 \in B_n) = p_m * q_n \text{ par indépendance}
.\]
Malheuresement : Ni les $ p_{m,n} $, ni les $ p_m $, ni les $ q_n $ ne sont connus ! $\rightarrow$ On vas estimer ces quantités et construire un test à partir de ces estimateur. 

\textbf{Notation:} Pour $ m \in \{1, \dots, M\} $ et $ n \in \{1, \dots, N\} $. On pose 
\begin{align*}
    N_{m,n} &= \sum_{i=1}^{T}\mathbbm{1}_{X_i \in A_m, Y_i \in B_n} \\
            &= \text{ effectif observé sur la classe } A_m * B_n \\
    N_{m, \centerdot} &= \sum_{i=1}^{T}\mathbbm{1}_{X_i \in A_m}\\
            &= \text{effectif total de } A_m \\
    N_{\centerdot, n} &= \sum_{i=1}^{T}\mathbbm{1}_{Y_i \in B_m} \\
        &= \text{effectif totale de } B_n
\end{align*}

\begin{rem}[]
    On a immédiatement 
    \[
        T = \sum_{n=1}^{N}N_{\centerdot, n} = \sum_{m=1}^{M}N_{m,\centerdot} = \sum_{n=1}^{N}\sum_{m=1}^{M}N_{m,n}
    .\]
\end{rem}
Estimateurs : Pour $ m \in \{1, \dots, M\} $ et $ n \in \{1,\dots, N\} $ 
\begin{align*}
    \hat{p}_{m,n} = \frac{N_{m,n}}{T} \approx p_{m,n} \text{ si } T \text{ est grand.} \\
    \hat{p}_{m} = \frac{N_{m, \centerdot}}{T} \approx p_m \text{ si } T \text{ est grand.} \\
    \hat{q}_{n} = \frac{N_{\centerdot,n}}{T} \approx q_n \text{ si } T \text{ est grand.}
\end{align*}


\subsubsection{Test du $ \mathcal{X}^2 $ d'indépendance}
\textbf{Données}: $ (X_1, Y_1), \dots, (X_T, Y_T) $ iid appariés. \begin{itemize}
    \item $ X_1 $ à valeur dans $ A_1, \dots, A_M $ 
    \item $ Y_1 $ à valeur dans $ B_1, \dots, B_N $ 
\end{itemize}

\textbf{Hypothèse}: 
\begin{itemize}
    \item $ H_0: X_1 \bot Y_1 $
    \item $ H_1: X_1 \not\bot Y_1 $
\end{itemize}

\textbf{Statistique de test}
\begin{align*}
    D &= T * \sum_{m=1}^{M}\sum_{n=1}^{N}\frac{(\hat{p}_{m,n} - \hat{p}_m \hat{q}_n)^2}{\hat{p}_m \hat{q}_n} \\
        &= \sum_{m=1}^{M}\sum_{n=1}^{N}\frac{( N_{m,n} - \frac{N_{m, \centerdot} N_{\centerdot, n}}{T})^2}{\frac{N_{m, \centerdot} N_{\centerdot, n}}{T}}
\end{align*}

\textbf{Condition:}
Si $ \forall m \in \{1,\dots, M\} $ et $ \forall n \in \{1, \dots, N\} $ 
\[
    T \hat{p}_m \hat{q}_m \geq 5
.\]
alors $ D $ suit approximativement une loi 
\begin{align*}
    &\mathcal{X}^2  (MN - 1 - (M-1) - (N-1)) \\
    \Leftrightarrow &\mathcal{X}^2  (MN - 1 - \text{ estimation de } p_m - \text{ Estimation des } q_n) \\
    \Leftrightarrow & \mathcal{X}^2 (MN-M-N+1) \\
    \Leftrightarrow & \mathcal{X}^2 ((M-1)(N-1))
\end{align*}

\textbf{Seuil de rejet}: Au niveau $ \alpha  $. \\
Soit $ h_\alpha  $ le quantile d'ordre $ 1-\alpha  $ de la loi $ \mathcal{X}^2((M-1)(N-1)) $. \\ 
Si $ D > h_\alpha  $, on rejette $ H_0 $. Sinon on conserve $ H_0 $. 

\textbf{Que se passe-t-il sous $ H_1 $ } \\
Si $ X_1 $ et $ Y_1 $ ne sont pas indépendants, il existe $ m_0 $ et $ n_0 $ tels que 
\[
    p_{m_0, n_0} \neq p_{m_0}q_{n_0}
.\]
Ainsi, 
\[
    \frac{(\hat{p}_{m_0, n_0} - \hat{p}_{m_0} \hat{q}_{n_0})^2 }{\hat{p}_{m_0} \hat{q}_{n_0}} \to _{T \to +\infty } \frac{(p_{m_0, n_0} - p_{m_0} q_{n_0})^2 }{p_{m_0} q_{n_0}}
.\]
Donc $ D \to + \infty  $ (on a multiplié par $ T $ )

\begin{exmp}[]
    Indépendance de la couleur des yeux et des cheveux.\\
    On a mesuré sur 1000 personnes leurs couleurs de yeux et cheveux qu'on a regroupé dans le tableau suivant.

    \begin{table}[!h]
        \centering
        \begin{tabular}{|l|l|l|l|l|l|}
        \hline
            yeux $ \backslash $  cheveux & Noirs ($A_1$) & Bruns & Blonds & Roux & Total \\ \hline
            Marrons ($B_1$) & $N_{1,1}=$ 152 & $N_{2,1}=$ 247 & 83 & 11 & $N_{\centerdot, 1}=$ 152 \\ \hline
            Vert ou Gris & 73 & 114 & 37 & 8 & 232 \\ \hline
            Bleus & 36 & 167 & 127 & 10 & 275 \\ \hline
            Total & $N_{1,\centerdot}=$ 261 & 463 & 247 & 29 & 1000 \\ \hline
        \end{tabular}
    \end{table}

    Condition : $ T * \hat{p}_m \hat{q}_n \geq 5 \Leftrightarrow T * \frac{N_{m, \centerdot} * N_{\centerdot, n}}{T*T} = $ effectif attendu
    
    $\rightarrow$ Vous calculez les conditions comme vous voulez (?)
    Tableau des effectifs attendu + regarder si on respecte les conditions
    \begin{table}[!h]
        \centering
        \begin{tabular}{|l|l|l|l|l|l|}
        \hline
            yeux $ \backslash $  cheveux & Noirs ($A_1$) & Bruns & Blonds & Roux & Total \\ \hline
            Marrons ($B_1$) & 128.67 & 228.26 & 121.77 & 14.3 & $N_{\centerdot, 1}=$ 152 \\ \hline
            Vert ou Gris & 60.55 & 107.42 & 57.3 & 6.73 & 232 \\ \hline
            Bleus & 71.78 & 127.32 & 67.93 & 7.98 & 275 \\ \hline
            Total & $N_{1,\centerdot}=$ 261 & 463 & 247 & 29 & 1000 \\ \hline
        \end{tabular}
    \end{table}

    Tous les effectifs attendus sont $ \geq 5 $ : On peut appliquer le test du $ \mathcal{X}^2 $ d'indépendance
    \begin{align*}
        D &= T \sum_{m=1}^{M}\sum_{n=1}^{N} \frac{(\hat{p}_{m,n} - \hat{p}_m \hat{q}_n)^2}{\hat{p}_m \hat{q}_n} \\
            &= \sum_{m=1}^{M}\sum_{n=1}^{N} \frac{(N_{m,n} - \frac{N_{m,\centerdot} N_{\centerdot, n}}{T})^2}{\frac{N_{m,\centerdot} N_{\centerdot, n}}{T}} \\
            &= \frac{(152 - 128.67)^2}{128.67} + \frac{(247 - 228.26)^2}{228.26} + \dots + \frac{(10 - 7.98)^2}{7.98} \\
            &= \text{ Une somme à 12 termes} \\
            &= 104.01
    \end{align*}

    \textbf{Zone de rejet}: Sous $ H_0, D \sim \mathcal{X}^2 (12 -1 3 - 2) = \mathcal{X}^2 (6)$. Pour un test au niveau 5\%, on lit le quantile d'ordre 95\% d'une $ \mathcal{X}^2(6) = 12.6 $ 

    \textbf{Conclusion}: $ D>12.6 $ On rejette $ H_0 $ couleur d'yeux et couleurs de cheveux ne sont pas indépendants
\end{exmp}


\subsection{Le $ \mathcal{X}^2 $ d'homogénéité}
\subsubsection{Pour deux échantillons}

\textbf{Données}: $ X_1, \dots, X_{n_1} $ et $ Y_1, \dots, Y_{n_2} $ deux échantillons iid indépendants entre eux (comme pour Kolmogorov-Smirnov). \\
Les variables sont toutes à valeurs dans les mêmes classes $ A_1, \dots, A_M $.

\textbf{Hypothèse}: On veut tester l'homogénéité \begin{itemize}
    \item $ H_0 = X_1$ et $ Y_1 $  ont la même loi $ \Leftrightarrow \forall m \in \{1,\dots,M \}, P(X_1 \in A_m) = P(Y_1 \in A_m) $ 
    \item $ H_1 = X_1$ et $ Y_1 $  n'ont pas la même loi $ \Leftrightarrow \exists m \in \{1, \dots, M\} $ tel que $ P(X_1 \in A_m) \neq P(Y_1 \in A_m) $ 
\end{itemize}

\begin{rem}[Lien entre test du $ \mathcal{X}^2 $ d'indépendance et d'homogénéité]
    On peut se ramener à un test d'indépendance en construisant l'échantillon apparié suivant $ i \leq n_1 + n_2 = T $ 
    \[
        (W_i, Z_i) = \begin{cases}
        (X_i, 1) &\text{ si } i \leq n_1 \\
        (Y_{i-n}, 2) &\text{ si } i > n_1 \\
        \end{cases} 
    .\]
    On est passé de :\begin{itemize}
        \item $ X_1, \dots, X_{n_1} $ à $ (X_1,1), (X_2, 1), \dots, (X_n, 1) $ 
        \item et $ Y_1, \dots, Y_{n_2} $ à $ (Y_1,2), (Y_2, 2), \dots, (Y_n, 2) $ 
    \end{itemize}
    On a : 
    \[
        W_1 \bot Z_1 \Leftrightarrow X_1 \text{ et } Y_1 \text{ ont la même loi.}
    .\]
    Pour tester l'homogénéité des deux population, il suffit de tester l'indépendance de $ Z_1 $ et $ W_1 $ \begin{itemize}
        \item $ W_1 $ est à valeur dans $ A_1, \dots, A_M $ 
        \item $ Z_1 $ est à valeur dans $ \{\{1\}, \{2\}\} $ 
    \end{itemize}

    $\rightarrow$ Vu comme ça le test se généralise très bien ! On peut l'utiliser pour comparer beaucoup plus d'échantillon ! (exo 5 TD6)
\end{rem}

\begin{exmp}[Mise en pratique sans l'indépendance]
    On va tester si $ P(X_1 \in A_m) = P(Y_1 \in A_m) \forall m \in \{1,\dots, M\}$.

    Sous $ H_0 $ les populations sont homogènes. On estime $ p_m = P(X_1 \in A_m) = P(Y_1 \in A_m) $ par 
    \[
        \hat{p}_m = \frac{N_m^X + N_m^X}{n_1 + n_2}
    .\]
    Avec $ N_m^X = \sum_{i=1}^{n_1}\mathbbm{1}_{X_i \in A_m} $ et $ N_m^Y = \sum_{j=1}^{n_2}\mathbbm{1}_{Y_j \in A_m} $.\\
    On pose alors $ \hat{p}_m^X = \frac{N_m^X}{n_1} $ et $ \hat{p}_m^Y = \frac{N_m^Y}{n_2} $. 

    \textbf{Statistique de test}: 
    \begin{align*}
        D &= n_1 \sum_{m=1}^{M}\frac{(\hat{p}_m^X - \hat{p}_m)^2}{\hat{p}_m} + n_2 \sum_{m=1}^{M}\frac{(\hat{p}_m^Y - \hat{p}_m)^2}{\hat{p}_m} \\
            &= \sum_{m=1}^{M}\frac{(N_m^X - n_1 \hat{p}_m)^2}{n_1 \hat{p}_m} + \sum_{m=1}^{M}\frac{(N_m^Y - n_2 \hat{p}_m)^2}{n_2 \hat{p}_m}
    \end{align*}
    Si $ \forall m \in \{1, \dots, M\}, n_1 \hat{p}_m \geq 5$ et $ n_2 \hat{p}_m \geq 5 $ alors $ D \sim \mathcal{X}^2 (M-1) $.

    \textbf{Seuil de rejet}: Au niveau $ \alpha  $, soit $ h_\alpha  $ le quantile d'ordre $ 1-\alpha  $ d'une $ \mathcal{X}^2(M-1) $. Si $ D > h_\alpha  $, on rejette $ H_0 $, sinon on conserve $ H_0 $.
\end{exmp}

\begin{exmp}[Groupe sanguins dans 2 populations]
    \begin{table}[!h]
        \centering
        \begin{tabular}{|l|l|l|l|l|l|}
        \hline
            Pop $\backslash$ Groupe & O & A & B & AB & Total \\ \hline
            Pop 1 & 121 & 120 & 79 & 33 & 353 = $n_1$ \\ \hline
            Pop 2 & 118 & 95 & 121 & 30 & 364 = $n_2$ \\ \hline
            Total & 239 & 215 & 200 & 63 & 717 \\ \hline
        \end{tabular}
    \end{table}
    Validité : $ n_1 \hat{p}_1, n_2 \hat{p}_1 \geq 5 $. Les calculs sont les mêmes ! 
    \begin{align*}
        D &= \frac{(121 - \frac{353*239}{717})^2}{\frac{353*239}{717}} + \frac{(120 - \frac{353*215}{717})^2}{\frac{353*215}{717}} + \dots + \frac{(118 - \frac{364*239}{717})^2}{\frac{364*239}{717}} + \dots + \frac{(30 - \frac{364*63}{717})^2}{\frac{364*63}{717}} \\
        & \approx  11.75
    \end{align*}
    On lit le quantile d'une loi $ \mathcal{X}^2 (3)$ et on décide. Faites le calcul et finissez. 
\end{exmp}

\begin{exmp}[Pour un nombre quelconque de population]
    \textbf{Donnée}: \begin{align*}
        X_1^{(1)}, \dots, X_{n_1}^{(1)} \\
        X_1^{(2)}, \dots, X_{n_1}^{(2)} \\
        \dots  \\
        X_1^{(K)}, \dots, X_{n_1}^{(K)} \\
    \end{align*}
    On a $ K $ échantillon indépendants de variable iid à valeur dans $ A_1, \dots, A_m $ 

    \textbf{Hypothèse}: \begin{itemize}
        \item $ H_0 $ Tout les échantillons ont la même loi
        \item $ H_1 $ Il existe un échantillons qui diffère des autres
    \end{itemize}

    \begin{rem}[]
        On peut créer l'échantillon apparié fictif :
        \[
            (W_i, Z_i) = (X_i^{(k)}, k)
        .\]
        et tester l'indépendance de $ Z_1 $ et $ W_i $  
    \end{rem}
    
    Ou bien on utilise 
    \begin{align*}
        N_m^{(k)} &= \sum_{i=1}^{n_k} \mathbbm{1}_{X_i^{(k)} \in A_m} \\
        \hat{p}_m &= \frac{N_m^{(1)} + \dots + N_m^{(k)}}{n_1 + \dots + n_k} \\
        D &= \sum_{h=1}^{K}\sum_{m=1}^{M} \frac{ (N_m ^{(k)} - n_k \hat{p}_m)^2 }{n_k \hat{p}_m}
    \end{align*}
    
    Condition : Si $ \forall k \leq L, \forall m \leq M : n_k \hat{p}_m \geq 5 $ alors $ D \sim \mathcal{X}^2 ((M-1)(K-1)) $. 

    Seuil de rejet : Quantile d'une $ \mathcal{X}^2 ( (M-1) (K-1) ) $. 

    Sous $ H_1, D \to + \infty  $ EXO (on avait plus le temps)
\end{exmp}

\underline{Nouveau cours du 10/03} \\

\section{Tests pour échantillons gaussiens}

\subsection{Rappels du cours de statistiques mathématiques}

\begin{thm}[Cochran]
    $ X_1, \dots, X_n $ v.a. iid. de loi $ \mathcal{N}(m, \sigma^2) $ alors 
    \begin{itemize}
        \item $ \bar{X}_n $  et $ V_n $ sont indépendant à 
        \[
            \bar{X}_n \frac{1}{n}\sum_{i=1}^{n}X_i , V_n = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X}_n)^2
        .\]
        \item 
        \[
            \frac{(n-1) V_n}{\sigma ^2} \sim \mathcal{X}^2(n-1)
        .\]
    \end{itemize}
\end{thm}

\begin{thm}[Student]
    $ X_1, \dots, X_n $ va. iid. $ \mathcal{N}(m, \sigma ^2) $ alors 
    \[
        \frac{\sqrt[]{n}}{\sqrt[]{V_n}} (\bar{X}_n - m) \sim \mathcal{T}(n-1)
    .\]
\end{thm}

\paragraph*{Rappel} \begin{itemize}
    \item La loi $ \mathcal{X}^2 (n) $ est la loi de 
    \[
        \sum_{i=1}^{n}Y_i^2 \text{ à } Y_i \text{ iid. } \mathcal{N}(0,1)
    .\]
    \item La loi $ \mathcal{T}(n) $ est la loi de 
    \[
        \frac{X}{\sqrt[]{V/n}} \text{ où } X \sim \mathcal{N}(0,1), V \sim \mathcal{X}^2(n)
    .\]
    \item Opération sur les gaussiennes : $ X \sim \mathcal{N}(m_1, \sigma_1 ^2), Y \sim \mathcal{N}(m_2, \sigma _2^2), X \bot Y $ \textbf{indépendant} alors : 
    \begin{align*}
        X + Y &\sim \mathcal{N}(m_1 + m_2, \sigma_1 ^2 + \sigma_2 ^2) \\
        X - Y &\sim \mathcal{N}(m_1 - m_2, \sigma_1 ^2 + \sigma_2 ^2) \\
        \lambda X &\sim \mathcal{N}(\lambda m_1, \lambda ^2 \sigma_1 ^2) \\
    \end{align*}
    \begin{rem}[]
        Que se passe-t-il si $ X $ et $ Y $ ne sont pas indépendantes ? \\
        $ X-Y $ n'est a priori pas gaussienne. Cependant si $ (X,Y) $ est gaussienn sur $ \mathbb{R}^2 $ (vecteur gaussien) alors $ X-Y $ est gaussien d'espérance $ m_1-m_2 $ mais de variance \textbf{inconnue}.

        $ (X,Y) $ gaussien si sa densité est de la forme 
        \[
            g(x,y) = \frac{1}{c}e^{-ax^2 - by^2 -2cxy}
        .\]
        \begin{exmp}[Pas à savoir et un peu dur]
            \begin{align*}
                X &\sim \mathcal{N}(0,1) \\
                Z &\sim \mathcal{N}(0,1) \bot X \\
                B &\sim Ber(\frac{1}{2})\\
                Y &= BX + (1-B)Z \sim \mathcal{N}(0,1 )\\
                X-Y &= \begin{cases}
                    0 &\text{ si } B=1\\
                    X-Z &\text{ si } B=0\\
                \end{cases} \\
                X - Y \text{ n'est clairement pas gaussienne}
            P(Y \leq t) &= P(Y \leq t \text{ et } B=1) + P(Y \leq t \text{ et } B =0) \\
                &= P(X \leq t) \frac{1}{2} + P(Z \leq t)\frac{1}{2} = P(X \leq 1)
            \end{align*}
        \end{exmp}
    \end{rem}
\end{itemize}

\subsection{Forme d'un test} 
\begin{itemize}
    \item Nom du test / sa fonction
    \item Type de données / Condition d'utilisation
    \item $ H_0, H_1 $ 
    \item Statistique de test : sous $ H_0 $ et $ H_1 $ 
    \item Forme de la zone de rejet et le seuil de rejet au niveau $ \alpha  $ 
\end{itemize}
\begin{rem}[]
    Je vous encourage fortement à revoir tous vos tests sous cette forme et à faire des fiche
\end{rem}

\subsection{Les test sur l'espérance}
Dans le TD7, vous avez vu plusieurs test "élémentaire" sur les données gaussiennes. Ils sont à connaître et sont succinctement rappelé ici

\subsubsection{Test sur la moyenne pour 1 échantillon gaussien de variance inconnue. Test de students à 1 échantillon}

\paragraph*{Données} $ X_1, \dots, X_n $ iid. $ \mathcal{N}(m, \sigma ^2) $ avec $ \sigma ^2 $ inconnu

\paragraph*{Hypothèse}\begin{itemize}
    \item $ H_0 = m = m_0$ 
    \item $ H_1 = m \neq m_0$ \textbf{(cas 1)} ou bien $ m>m_0 $ \textbf{(cas 2)} ou bien $ m < m_0 $ \textbf{(cas 3)}
\end{itemize}

\paragraph*{Statistique de test}
\[
    D = \frac{\sqrt[]{n}}{\sqrt[]{V_n}}(\bar{X}_n - m_0)
.\]
\begin{itemize}
    \item Sous $ H_0, D \sim \mathcal{T}(n-1)$ 
    \item Sous $ H_1 $ \begin{align*}
        D &= \frac{\sqrt[]{n}}{\sqrt[]{V_n}} (\bar{X}_n - m) + \frac{\sqrt[]{n}}{\sqrt[]{V_n}} \\
            &= \mathcal{T}(n-1) + \text{ Biais ? du signe de } m_1 - m_0
    \end{align*}
\end{itemize}

\paragraph*{Zone de rejet pour le cas 1}
Soit $ h_{\alpha /2} $ le quantile d'ordre $ \frac{\alpha }{2} $ et $ h_{1-\alpha /2} $ le quantile d'ordre $ 1 - \frac{\alpha }{2} $ de la loi $ \mathcal{T}(n-1) $ 

Si $ D > h_{1 - \alpha /2} $ ou $ D < h_{\alpha /2} $, on rejette $ H_0 $ 
\begin{rem}[\textbf{Attention}]
    Comme $ h_{\alpha /2} = - h_{1-\alpha /2} $ cela se ré-écrit $ \left| D \right| > h_{1 - \alpha /2} $ 
\end{rem}

\paragraph*{Zone de rejet pour le cas 2}
\[
    D = \mathcal{T}(n-1) + \frac{\sqrt[]{n}}{\sqrt[]{V_n}}(m-m_0) \text{ (biais > 0)}
.\]
Soit $ h_{1 - \alpha } $ le quantile d'ordre $ 1 - \alpha  $ d'une loi $ \mathcal{T}(n-1)$. \\
Si $ D > h_{1-\alpha } $ on rejette $ H_0 $. Sinon on conserve $ H_0 $. 

\paragraph*{Zone de rejet pour le cas 3}
Sous $ H_1 = m < m_0, D $ prend des valeurs plutôt négatives. Soit $ H_\alpha  $ le quantile d'ordre $ \alpha  $ d'une $ \mathcal{T}(n-1) $. \\
Si $ D < h_\alpha $ on rejette $ H_0 $ sinon conserver $ H_0 $. 

\subsubsection{Test sur des moyenne pour 2 échantillons gaussiens appariés}
\paragraph*{Données} \begin{itemize}
    \item $ X_1, \dots, X_n  $ iid. $ \mathcal{N}(m_1, \sigma_1 ^2), \sigma_1^2 $ inconnus
    \item $ Y_1, \dots, Y_n  $ iid. $ \mathcal{N}(m_2, \sigma_2 ^2), \sigma_2^2 $ inconnus
    \item Échantillon apparié $ (X_i, Y_i) $ indépendant si $ i \neq j $ mais $ X_i $ n'est pas indépendant de $ Y_i $ 
\end{itemize}

\paragraph*{Hypothèse}
\begin{itemize}
    \item $ H_0 = m_1 = m_2 $ 
    \item $ H_1 = m_1 \neq m_2 $ ou $ m_1 > m_2 $ ou $ m_1 < m_2 $ 
\end{itemize}

\paragraph*{Statistique de test}
Si on pose $ \bar{Z}_n = \frac{1}{n} \sum_{i=1}^{n}Z_i $ et $ V_n = \frac{1}{n-1}\sum_{i=1}^{n}(Z_i - \bar{Z}_n)^2 $ 
\[
    D = \frac{\sqrt[]{n}}{\sqrt[]{V_n}}\bar{Z}_n
.\]

\paragraph*{Zone de rejet}
\begin{itemize}
    \item Sous $ H_0, D \sim \mathcal{T}(n-1) $
    \item Sous $ H_1 $ à completer vous-même, c'est le même que le test précédent
\end{itemize}

\textbf{Vois exo2 du TD7 : }
\paragraph*{Correction de l'exo2 du TD7} On suppose que les va $ Z_i $ sont iid gaussiennes ! donc \begin{align*}
    E(Z_i) &= E(X_i) - E(Y_i) = m_1 - m_2 \\
    Var(Z_i) &= ........ \text{MANQUE DES TRUCS DEMANDER A JESS }
\end{align*}

Zone de rejet de niveau $ \alpha  $ : 
\[
    \mathcal{R} = \{\left| T \right| \geq t_{n-1} (1- \alpha /2)\}
.\]
Sous $ H_0, T \sim \mathcal{T}(n-1) $. \\
Application : au niveau 10\% $ \mathcal{R}=\{T \geq 1.3\} $ et $ T = \sqrt[]{46}\frac{1.5}{\sqrt[]{8}} = 3.6 $ Donc on rejette $ H_0 $ : la note de stat math est plus grande que la note de simulation.


\subsubsection{Test d'égalité des moyennes pour 2 échantillons gaussiens indépendant de variance connues}

\paragraph*{Données}
\begin{itemize}
    \item $ X_1, \dots, X_{n_1}  $ iid. $ \mathcal{N}(m_1, \sigma_1 ^2), \sigma_1^2 $ connue
    \item $ Y_1, \dots, Y_{n_2}  $ iid. $ \mathcal{N}(m_2, \sigma_2 ^2), \sigma_2^2 $ connue
    \item $ (X_1, \dots, X_{n_1}) \bot (Y_1, \dots, Y_{n_2}) $ 
\end{itemize}

\paragraph*{Hypothèse}
\begin{itemize}
    \item $ H_0 = m_1 = m_2 $ 
    \item $ H_1 = m_1 \neq m_2 $ ou $ m_1 > m_2 $ ou $ m_1 < m_2 $ 
\end{itemize}

\paragraph*{Statistique de test}
Voir TD7 exo 3: Correction exo 3 TD7 \\
\begin{enumerate}
    \item $ \bar{X}_{n_1} \sim \mathcal{N}(m_1, \frac{\sigma _1^2}{n_1}) $ et $ \bar{Y}_{n_2} \sim \mathcal{N}(m_2, \frac{\sigma _2^2}{n_2}) $ 
    \item $ \bar{X}_{n_1} - \bar{Y}_{n_2} \sim \mathcal{N}(m_1-m_2, \frac{\sigma _1^2}{n_1} + \frac{\sigma _2^2}{n_2}) $ 
    \item $ H_0 : m_1 = m_2, H_1 = m_1 \neq m_2, T = \bar{X}_{n_1} - \bar{Y}_{n_2}, \mathcal{R} = \{\left| T \right| \geq C_\alpha \}$ car sous $ H_0, T \sim \mathcal{N}(0, \frac{\sigma _1^2}{n_1}+\frac{\sigma _2^2}{n_2}) $
    \begin{align*}
        P(\left| \bar{X}_{n_1} - \bar{Y}_{n_2} \right| \geq C_\alpha ) &= P(\frac{\left| \bar{X}_{n_1} - \bar{Y}_{n_2} \right|}{\sqrt[]{\frac{\sigma _1^2}{n_1} + \frac{\sigma _2^2}{n_2}}} \geq \frac{C_\alpha }{\frac{\sigma _1^2}{n_1} + \frac{\sigma _2^2}{n_2}}) \\
        &= \phi ^{-1} (1 - \alpha /2)
    \end{align*}
    donc $ c_\alpha = \phi ^{-1} (1 - \frac{\alpha }{2}) \sqrt[]{\frac{\sigma _1^2}{n_1} + \frac{\sigma _2^2}{n_2}} $ 
    \item Application : \begin{itemize}
        \item Lille : 10 mesures, $ \sigma ^2 = 4 $ 
        \item Sydney : 20 mesures, $ \sigma ^2 = 9 $ 
        \item $ \bar{X}_{n_1} - \bar{Y}_{n_2} = 1 $
        \item Sous $ H_0, \bar{X}_{n_1} - \bar{Y}_{n_2} \sim \mathcal{N}(0, \frac{4}{10} + \frac{9}{20} = 0.85) $ 
        \item Sous $ H_1, \bar{X}_{n_1} - \bar{Y}_{n_2} $ est plus grand que sous $ H_0 $ 
        \[
            \mathcal{R} = \{\bar{X}_{n_1} - \bar{Y}_{n_2} \geq c_\alpha \}
        .\]
        \begin{align*}
            P_{H_0}(\bar{X}_{n_1} - \bar{Y}_{n_2} \geq C_\alpha ) &= P_{H_0}( \frac{\bar{X}_{n_1} - \bar{Y}_{n_2}}{\frac{\sigma _1^2}{n_1} + \frac{\sigma _2^2}{n_2}} \geq \frac{c_\alpha }{\frac{\sigma _1^2}{n_1} + \frac{\sigma _2^2}{n_2}})
        \end{align*}
        donc $ \frac{c_\alpha }{\frac{\sigma _1^2}{n_1} + \frac{\sigma _2^2}{n_2}} = 2.06 \Leftrightarrow c_\alpha = 2.06 \sqrt[]{0.85} = 1.89 $ \\
        Donc on conserve $ H_0 $, les deux composé peuvent avoir la même masse.
    \end{itemize}
\end{enumerate}

\paragraph*{Zone de rejet}

\subsection{Test sur les variances}

\subsubsection{Test d'égalité des variances pour un échantillon gaussien de moyenne inconnus}

\paragraph*{Données} $ X_1, \dots, X_n $ iid. $ \mathcal{N}(m,\sigma ^2) $ avec $ m, \sigma ^2 $ inconnus

\paragraph*{Hypothèse}
\begin{itemize}
    \item $ H_0 = \sigma ^2 = \sigma_0 ^2 $ 
    \item $ H_1 = \sigma ^2 \neq \sigma_0 ^2 $ ou $ \sigma ^2 > \sigma_0 ^2 $ ou $ \sigma ^2 < \sigma_0 ^2 $ 
\end{itemize}

\paragraph*{Statistique de test}
\[
    D = \frac{(n-1)}{\sigma_0 ^2}V_n
.\]

\paragraph*{Zone de rejet} à completer (attention $ \mathcal{X}^2 $ pas symétrique )

\subsubsection{Test de comparaison des variances de Fisher}
\begin{defn}[Loi de Fisher]
    Soit $ V \sim \mathcal{X}^2(d_1), W \sim \mathcal{X}^2(d_2), V \bot W $. La loi de $ \frac{V/d_1}{W/d_2} $ est appelée loi de Fisher à $ (d_1, d_2) $ degrés de liberté.
    \begin{rem}[]
        Cette loi est tabulée pour $ d_1 $ et $ d_2 $ pas trop grands. \begin{itemize}
            \item Elle admet une densité
            \item Elle est importante car elle sert souvent (en ANOVA notamment)
        \end{itemize}
    \end{rem}
    On la note $ \mathcal{F}(d_1, d_2) $ 
\end{defn}

\paragraph*{Données}\begin{itemize}
    \item $ X_1, \dots, X_{n_1}  $ iid. $ \mathcal{N}(m_1, \sigma_1 ^2)$ $ m_1 $ et $ \sigma_1^2 $ inconnus
    \item $ Y_1, \dots, Y_{n_2}  $ iid. $ \mathcal{N}(m_2, \sigma_2 ^2)$ $ m_2 $ et $ \sigma_2^2 $ inconnus
    \item $ (X_1, \dots, X_{n_1}) \bot (Y_1, \dots, Y_{n_2})$ échantillons indépendant
\end{itemize}

\paragraph*{Hypothèse}
\begin{itemize}
    \item $ H_0 = \sigma _1^2 = \sigma _2^2 $ 
    \item $ H_1 = \sigma _1^2 \neq \sigma _2^2 $ ou $\sigma _1^2 > \sigma _2^2 $ ou $ \sigma _1^2 < \sigma _2^2 $
\end{itemize}

\paragraph*{Statistique de test}
\[
    D = \frac{V_{n_1}^X}{V_{n_2}^Y}
.\]
avec $ V_{n_1}^X = \frac{1}{n_1 - 1} \sum_{i=1}^{n_1}(X_i - \bar{X}_{n_1})^2 $ et $ V_{n_2}^Y = \frac{1}{n_2 - 1} \sum_{i=1}^{n_2}(Y_i - \bar{Y}_{n_2})^2 $ 

\paragraph*{Zone de rejet}
\begin{itemize}
    \item Sous $ H_0, \sigma ^2 = \sigma _1^2 = \sigma _2^2 $ : 
    \[
        D = \frac{\frac{V_{n_1}^X (n-1)}{\sigma ^2} \frac{1}{n_1-1}}{\frac{V_{n_2}^Y (n_2 -1)}{\sigma ^2} \frac{1}{n_2-1}} \sim \mathcal{F}(n_1 - 1, n_2 - 1) \text{ par Cochran}
    .\]
    \item Sous $ H_1, \sigma _1^2 \neq \sigma _2^2 $ 
    \[
        D = \mathcal{F}(d_1, d_2) * \frac{\sigma _1^2}{\sigma _2^2}
    .\]
\end{itemize}
\begin{itemize}
    \item \textbf{Cas 1 : $ H_1: \sigma _1^2 \neq \sigma _2^2 $ } : Soit $ h_{\alpha /2} $ le quantile $ \frac{\alpha }{2} $ d'une $ \mathcal{F}(n_1-1, n_2 - 1) $  et $ h_{1- \alpha /2} $ le quantile d'ordre $ 1 - \frac{\alpha }{2} $ d'une $ \mathcal{F}(n_1 - 1, n_2 - 1) $. \\
    Si $ D > h_{1 - \alpha /2} $ ou bien $ D < h_{\alpha /2} $ on rejette $ H_0 $. Sinon on conserve $ H_0 $ 

    \item \textbf{Cas 2 : $ H_1: \sigma _1^2 > \sigma _2^2 $ } : Soit $ h_{1 - \alpha} $ le quantile $ 1 -\alpha  $ d'une $ \mathcal{F}(n_1-1, n_2 - 1) $ \\
    Si $ D > h_{1 - \alpha} $ on rejette $ H_0 $. Sinon on conserve $ H_0 $.

    \item \textbf{Cas 3 : $ H_1: \sigma _1^2 < \sigma _2^2 $ } : Soit $ h_{\alpha} $ le quantile $ \alpha $ d'une $ \mathcal{F}(n_1-1, n_2 - 1) $ \\
    Si $ D < h_{\alpha} $ on rejette $ H_0 $. Sinon on conserve $ H_0 $.
\end{itemize}

\subsubsection{Test de Student à 2 échantillons : Test de comparaison des moyenne de 2 échantillons gaussiens indépendants de variance égale}

\paragraph*{Données}
\begin{itemize}
    \item $ X_1, \dots, X_{n_1}  $ iid. $ \mathcal{N}(m_1, \sigma ^2), m_1 $ inconnus
    \item $ Y_1, \dots, Y_{n_2}  $ iid. $ \mathcal{N}(m_2, \sigma ^2), m_2 $ inconnus (même variance)
    \item Échantillon indépendant $ (X_1, \dots, X_{n_1}) \bot (Y_1, \dots, Y_{n_2}) $ 
\end{itemize}

\paragraph*{Hypothèse}
\begin{itemize}
    \item $ H_0 = m_1 = m_2 $ 
    \item $ H_1 = m_1 \neq m_2 $ ou $ m_1 > m_2 $ ou $ m_1 < m_2 $ 
\end{itemize}

\paragraph*{Statistique de test}
\[
    D = \frac{1}{\sqrt[]{\frac{1}{n_1} + \frac{1}{n_2}}} \frac{\bar{X}_{n_1} - \bar{Y}_{n_2}}{\sqrt[]{W}}
.\]
avec $ W = \frac{(n_1 - 1) V_{n_1}^X + (n_2 - 1) V_{n_2}^Y}{n_1 + n_2 - 2} $. 

\paragraph*{Zone de rejet}
\begin{itemize}
    \item Sous $ H_0 : m_1 = m_2$ \begin{align*}
        \bar{X}_{n_1} \sim \mathcal{N}(m_1, \frac{\sigma ^2}{n_1}) \\
        \bar{Y}_{n_2} \sim \mathcal{N}(m_2, \frac{\sigma ^2}{n_2}) \\
        \bar{X}_{n_1} - \bar{Y}_{n_2} \sim \mathcal{N}(0, \sigma ^2 (\frac{1}{n_1} + \frac{1}{n_2}))
    \end{align*}
    Donc 
    \[
        \frac{1}{\sqrt[]{\frac{1}{n_1} + \frac{1}{n_2}} \sigma } (\bar{X}_{n_1} - \bar{Y}_{n_2}) \sim \mathcal{N}(0,1)
    .\]
    On a déjà un terme de la stat de test, ce qu'il reste 
    \begin{align*}
        \frac{(n_1 - 1) V_{n_1}^X}{\sigma ^2} &+ \frac{(n_2 - 1)V_{n_2}^Y}{\sigma ^2} &\sim \mathcal{X}^2 (n_1 + n_2 - 2) \\
        \mathcal{X}^2(n_1 -1) & \bot \mathcal{X}^2(n_2 - 1) &
    \end{align*}
    \begin{align*}
        \sqrt[]{W} &= \frac{\sigma }{\sigma } \frac{\sqrt[]{(n_1 - 1) V_{n_1}^X + (n_2 -1) V_{n_2}^Y}}{\sqrt[]{n_1 + n_2 - 2}} \\
            &= \sigma \frac{\sqrt[]{\mathcal{X}^2(n_1 + n_2 - 2)}}{\sqrt[]{n_1 + n_2 - 2}}
    \end{align*}
    Ainsi sous $ H_0 $ 
    \[
        D =^{loi} \frac{\mathcal{N}(0,1)}{\sqrt[]{\frac{\mathcal{X}^2(n_1 + n_2 - 2)}{n_1 + n_2 - 2}}} \sim \mathcal{T}(n_1 + n_2 - 2)
    .\]
    CCL à retenir : sous $ H_0 $ 
    \[
        D \sim \mathcal{T}(n_1 + n_2 - 2)
    .\]
    \item Sous $ H_1 $ : \begin{itemize}
        \item Si $ m_1 > m_2 $, D prend des valeurs plus grades qu'une Student à $ n_1 + n_2 -2 $ degrés de libertés
        \item Si $ m_1 < m_2 $, D prend des valeurs négatives
    \end{itemize}
\end{itemize}

\begin{itemize}
    \item \textbf{Cas 1 : $ H_1: m_1 \neq m_2 $ } : Soit $ h_{\alpha /2} $ le quantile $ \frac{\alpha }{2} $ d'une $ \mathcal{T}(n_1 + n_2 - 2) $  et $ h_{1- \alpha /2} $ le quantile d'ordre $ 1 - \frac{\alpha }{2} $ d'une $ \mathcal{T}(n_1 + n_2 - 2) $. \textbf{Attention} $ h_{\alpha /2} = - h_{1 - \alpha /2} $ \\
    Si $ \left| D \right| > h_{1 - \alpha /2} $ on rejette $ H_0 $. Sinon on conserve $ H_0 $ 

    \item \textbf{Cas 2 : $ H_1: m_1 > m_2 $ } : Soit $ h_{1 - \alpha} $ le quantile $ 1 -\alpha  $ d'une $ \mathcal{T}(n_1 + n_2 - 2) $ \\
    Si $ D > h_{1 - \alpha} $ on rejette $ H_0 $. Sinon on conserve $ H_0 $.

    \item \textbf{Cas 3 : $ H_1: m_1 < m_2 $ } : Soit $ h_{\alpha} $ le quantile $ \alpha $ d'une $ \mathcal{T}(n_1 + n_2 - 2) $ \\
    Si $ D < h_{\alpha} = - h_{1 - \alpha } $ on rejette $ H_0 $. Sinon on conserve $ H_0 $.
\end{itemize}

\subsubsection{Test de Welch : Le test de Student se généralisant au cas des variances non égales}
\paragraph*{Données}
\begin{itemize}
    \item $ X_1, \dots, X_n  $ iid. $ \mathcal{N}(m_1, \sigma_1 ^2), m_1 \text{ et }\sigma_1^2 $ inconnus
    \item $ Y_1, \dots, Y_n  $ iid. $ \mathcal{N}(m_2, \sigma_2 ^2), m_2 \text{ et }\sigma_2^2 $ inconnus
    \item Échantillons indépendants
\end{itemize}

\paragraph*{Hypothèse}
\begin{itemize}
    \item $ H_0 = m_1 = m_2 $ 
    \item $ H_1 = m_1 \neq m_2 $ ou $ m_1 > m_2 $ ou $ m_1 < m_2 $ 
\end{itemize}

\paragraph*{Statistique de test}
\[
    D = \frac{\bar{X}_{n_1} - \bar{Y}_{n_2}}{\sqrt[]{\frac{V_{n_1}^X}{n_1} + \frac{V_{n_2}^Y}{n_2}}}
.\]

\paragraph*{Zone de rejet}
\begin{itemize}
    \item Sous $ H_0, D $ suit \textbf{approximativement} une loi $ \mathcal{T}(\mu ) $. $ \mu  $ n'est pas connu et est approximé par des formules horribles
    \item Sous $ H_1 $ 
\end{itemize}

\underline{Nouveau cours du 17/03} \\ 

\section{Test de Mann-Whitney-Wilioxon ou test de la somme des rangs}

\begin{defn}[Ordre Stochastique]
    Soit $ X $ et $ Y $ deux variables aléatoires, on dit que $ Y $ domine stochastiquement $ X $ si 
    \[
        \forall t \in \mathbb{R}, F_Y(t) \leq F_X(t)
    .\]
    Cela équivaut à 
    \[
        \forall t \in \mathbb{R}, P(X>t) \leq P(Y > t)
    .\]
    Si $ Y\succ X $ et $ Y \neq X $ alors 
    \[
        E(Y) > E(X)
    .\]
    et si on note $ m_X $ et $ m_Y $ les médianes de $ X $ et $ Y $ on a également 
    \[
        m_X \leq m_Y
    .\]
\end{defn}

Ce test est proche de KS à deux échantillons, en pratique il est même mieux.
\paragraph*{Données}
\begin{itemize}
    \item $ X_1, \dots, X_{n_1}  $ iid. 
    \item $ Y_1, \dots, Y_{n_2}  $ iid.
    \item Échantillons indépendants
    \item On suppose que $ F_X $ et $ F_Y $ sont \textbf{continues}.
\end{itemize}

\paragraph*{Hypothèse}
\begin{itemize}
    \item $ H_0 = X_1 $ et $ Y_1 $ ont la même loi. $ F_{X_1} = F_{Y_1} $ 
    \item $ H_0 = X_1 $ et $ Y_1 $ n'ont pas la même loi. $ F_{X_1} \neq F_{Y_1} $ \begin{itemize}
        \item Ou $ X_1 \succ Y_1 $ C'est à dire $ F_{X_1} \neq F_{Y_1} $ et $ \forall t \in \mathbb{R}, F_{Y_1}(t) \leq F_{X_1}(t) $ 
        \item Ou $ Y_1 \succ X_1 $ C'est à dire $ F_{X_1} \neq F_{Y_1} $ et $ \forall t \in \mathbb{R}, F_{X_1}(t) \leq F_{Y_1}(t) $ 
    \end{itemize}
\end{itemize}

\paragraph*{Statistique de test}
On note $ n = n_1 + n_2 $. On crée le vecteur 
\[
    Z = (X_1, \dots, X_{n_1}, Y_1, \dots, Y_{n_2}) = (Z_1, \dots, Z_n)
.\]
Z est la concaténation des deux échantillons. On ordonne Z par ordre croissant $ Z^\prime = (Z_{(1)}, \dots, Z_{(n)}) $ et on pose $ \forall i \in \{1,\dots, n_1\} $ 
\begin{align*}
    R(i) &= \text{ Rang de } X_i \text{ dans } Z^\prime  \\
        &= \sum_{j=1}^{n}\mathbbm{1}_{X_i \leq Z_j}
\end{align*}
On pose finalement la stat de test suivant
\[
    U = \sum_{i=1}^{n_1}R(i) = \text{ la somme des rangs des } X_i \text{ dans } Z^\prime 
.\]
\begin{rem}[]
    En cas d'ex-æquo, on leur attribue le rang moyen des rangs. Voir exemple. \\
    C'est ici la puissance de ce test par rapport à KS, il départage les ex-æquo d'une manière mathématique, contrairement à KS
\end{rem}

En général $ U $ est à valeurs entre 
\[
    1 + 2 + \dots + n_1 = \frac{n_1 (n_1 + 1)}{2} \text{ et } (n_2+1) + (n_2 + 2) + \dots + n = n_1(n_2 + \frac{n_1 + 1}{2})
.\]
(réfléchir au cas les plus extremes)

\paragraph*{Zone de rejet}
\begin{itemize}
    \item Sous $ H_0 $ : Quel est le rang de $ X_i $ ? $ R(1) $ est une variable. uniforme sur $ \{1,\dots,n\} $. Sous $ H_0 $, les $ X_i $ sont uniformément répartis dans le vecteur $ Z^\prime  $, et cela indépendamment de leur loi. Tout ce qui compte, c'est que le vecteur $ Z $ sont un vecteur de variables iid. \\
    \textbf{CCL:} Ainsi, sous $ H_0 $, la loi de U ne dépend pas de la loi de $ X_1 $ et $ Y_1 $. Elle ne dépend que de $ n_1 $ et $ n_2 $. On peut alors tabuler la variable $ U $.
    \item \begin{itemize}
        \item Sous $ H_1 = Y_1 \succ X_1$, les $ X_i $ sont plutôt au début du vecteur $ Z^\prime $, $ U $ prend donc de petites valeurs.
        \item Si $ H_1 = X_1 \succ Y_1 $, les $ X_i $ sont plutôt à la fin du vecteur $ Z^\prime  $. $ U $  prend donc des grandes valeurs.
        \item Si $ H_1 : F_{X_1} \neq F_{Y_1} $, $ U $ va prendre des valeurs extremes, mais on ne sait pas de quel côté.
    \end{itemize}
\end{itemize}

La loi de $ U $ est \textbf{symétrique} par rapport à $ n_1 \frac{n+1}{2} $ 
\begin{figure}[!h]
    \centering
    \includegraphics*[width=.6 \textwidth]{fig6.jpg}
\end{figure}

\begin{itemize}
    \item \textbf{Cas 1} $ H_1: Y_1 \succ X_1 $ au niveau $ \alpha $, on pose $ H_\alpha  $ le quantile d'ordre $ \alpha $ de la loi $ U(n_1, n_2) $. Si $ U < h_\alpha  $ on rejette $ H_0 $ sinon on conserve $ H_0 $.
    \item \textbf{Cas 2} $ H_1 : X_1 \succ Y_1 $ au niveau $ \alpha  $ on pose $ h_{1 - \alpha } $ le quantile d'ordre $ 1 - \alpha  $ de la loi de $ U(n_1, n_2) $ Si $ U > h_{1 - \alpha } $, on rejette $ H_0 $ sinon on conserve $ H_0 $.
    \item \textbf{Cas 3} $ H_1 : F_{X_1} \neq F_{Y_1} $ au niveau $ \alpha $ on pose $ h_{\alpha /2} $ et  $ h_{1 - \alpha/2 } $ les quantiles d'ordre $ \alpha /2 $ et $ 1 - \alpha/2  $ de la loi de $ U(n_1, n_2) $ Si $ U < h_{\alpha /2} $ ou $ U > h_{1 - \alpha /2} $ , on rejette $ H_0 $ sinon on conserve $ H_0 $.
\end{itemize}

\begin{exmp}[]
    On a 2 échantillons : \begin{itemize}
        \item 8 étudiants qui viennent en amphi notes : $ 15, 16.4, 5.6, 16.4, 18.8, 15.6, 15.2, 12.8 $ 
        \item 9 étudiants qui ne viennent pas en amphi : $ 15.6, 11, 12.6, 7.4, 9.6, 14.8, 13, 15.4, 12.6 $ 
    \end{itemize}
    Est-ce que la présence en amphi à un impact sur les notes ? \begin{itemize}
        \item $ H_0 $ pas d'impact = même loi
        \item $ H_1 $ impact : lois différentes
    \end{itemize}
    On choisit d'effectuer le test de Mann-Whitney (On n'utilise pas KS cas on a plusieurs notes répété). 

    On trie les données :
    \begin{table}[!ht]
        \centering
        \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
        \hline
            Obs & \textbf{5.6} & 7.4 & 9.6 & 11 & 12.6 & 12.6 & \textbf{12.8} & 13 &  \\ \hline
            Rang & 1 & 2 & 3 & 4 & 5.5 & 5.5 & 7 & 8 &  \\ \hline
            Obs & 14.8 & \textbf{15} & \textbf{15.2} & 15.4 & \textbf{15.6} & 15.6 & \textbf{16.4} & \textbf{16.4} & \textbf{18.8} \\ \hline
            Rang & 9 & 10 & 11 & 12 & 13.5 & 13.5 & 15.5 & 15.5 & 17 \\ \hline
        \end{tabular}
    \end{table}

    On calcule
    \begin{align*}
        U &= 1 + 7 + 10 + 11 + 13.5 + 15.5 + 15.5 + 17 \\ 
            &= \text{Somme des rangs de } X_i = 90.5
    \end{align*}

    Ici $ n_1 = 8 $ et $ n_2 = 9 $, niveau $ 5\% $, on retrouve
    \begin{align*}
        h_{0.025} &\approx 51 \\
        h_{0.975} &= 51 + 2(72 - 51) \\
            &= 93
    \end{align*}
    Revoir le graphique précédent. 

    Zone de rejet : Comme $ U \geq 51 $ et $ U \leq 93 $, on conserve $ H_0 $.
\end{exmp}

\paragraph*{Bilan de ce test} Ce test est une alternative au test d'homogénéité de KS.
\begin{itemize}
    \item KS détecte n'importe quelle différence de loi. 
    \item MW est plus sensible à des changement de médiane, plutôt des translations.
    \item MW est plus utilisé et apprécié.
    \item MW gère les ex-æquo. Alors que KS déteste les ex-æquo.
    \item MW pas ouf si juste un changement de variance et pas de médiane.
    \item Si $ n_1, n_2 $ sont grands. On n'a pas la table, on utilise alors le test asymptotique.
    \[
        \frac{U - E(U)}{\sqrt[]{Var(U)}} = \frac{U - n \frac{n_1 + 1}{2}}{\sqrt[]{\frac{n_1 n_2 (n+1)}{12}}} \to Z \sim \mathcal{N}(0,1)
    .\]
\end{itemize}

\section{Test du signe et test du signe et rang de Wilcoxon}
\paragraph*{Données}
\begin{itemize}
    \item $ X_1, \dots, X_{n_1}  $ iid. 
    \item $ Y_1, \dots, Y_{n_2}  $ iid. 
    \item Échantillon \textbf{appariées} $ (X_1, Y_1), \dots, (X_n, Y_n) $ sont iid $ X_1 \not \bot Y_1 $ 
\end{itemize}
On note $ Z_i = Y_i - X_i $. On suppose que $ Z_i $  a une fonction de répartition continue donc aucun des $ Z_i $ ne vaut 0.

\subsection{Test du signe / test de la médiane}

\paragraph*{Hypothèse}
\begin{itemize}
    \item $ H_0 $ La médiane de $ Z $ vaut 0. $ m_Z = 0 $. C'est à dire que $ P(Y_1 < X_1) = 1/2 $ 
    \item $ H_1 = m_Z \neq 0 $ ou $ m_Z > 0 \Leftrightarrow P(Z \leq 0) > 1/2 \Leftrightarrow P(Y_1 > X_1) > 1/2$ ou $ m_Z < 0 $ 
\end{itemize}

\paragraph*{Statistique de test}
\begin{align*}
    S_n &= \sum_{i=1}^{n}\mathbbm{1}_{Z_i \leq 0} \\
    &= \text{ Nombre de } Y_i > X_i
\end{align*}

\begin{itemize}
    \item Sous $ H_0 : P(Z_i > 0) = P(Y_i > X_i) = \frac{1}{2} $ 
    \[
        \mathbbm{1}_{Z_i > 0} \sim Ber(\frac{1}{2})
    .\]
    donc 
    \[
        S_n \sim Bin(n,\frac{1}{2})
    .\]

    \item Sous $ H_1 $ \begin{itemize}
        \item Si $ m_z > 0, P(Y_i > X_i) > \frac{1}{2}, S_n \sim Bin(n,p), p> \frac{1}{2} $ donc $ S_n $ est "grand"
        \item Si $ m_z < 0, P(Y_i < X_i) > \frac{1}{2} $ donc $ S_n $ est petit.
        \item Si $ m_z \neq  0, S_n $ a un comportement proche des extremes (petit/grand).
    \end{itemize}
\end{itemize}

\paragraph*{Zone de rejet}
Au niveau $ \alpha  $ \begin{itemize}
    \item Si $ H_1 : m_Z > 0 $. Soit $ h_{1 - \alpha } $ le quantile d'ordre $ 1 - \alpha  $ de la loi $ Bin(n,\frac{1}{2}) $. Si $ S_n > h_{1 - \alpha } $ on rejette $ H_0 $ sinon on conserve $ H_0 $ 
    \item Si $ H_1 : m_Z < 0 $. Soit $ h_{\alpha } $ le quantile d'ordre $ \alpha  $ de la loi $ Bin(n,\frac{1}{2}) $. Si $ S_n < h_{\alpha } $ on rejette $ H_0 $ sinon on conserve $ H_0 $ 
    \item Si $ H_1 : m_Z \neq 0 $. Soit $ h_{\alpha/2 } $ et $ h_{1 - \alpha /2} $  le quantile d'ordre $ \alpha /2 $ et $ 1 - \alpha /2 $ de la loi $ Bin(n,\frac{1}{2}) $. Si $ S_n < h_{alpha /2 } $ ou $ S_n > h_{1 - \alpha /2} $ on rejette $ H_0 $ sinon on conserve $ H_0 $ 
\end{itemize}
\begin{rem}[En cas d'égalité ]
    Si il existe $ i $ tel que $ Z_i = 0 $. On exclut ces données et on recommence avec le reste $ n \to n-1 $ 
\end{rem}

\paragraph*{CCL sur le test}
\begin{itemize}
    \item Ce test ne regarde \textbf{que} le signe et pas les amplitudes. (exemple : on peut avoir 5 fois $ -100 $ dans nos données et 5 fois $ +1 $ c'est la même chose). En général on va appliquer ce test quand \textbf{on ne connaît pas} les amplitudes mais juste les signes ("Est ce que la situation c'est améliorer ?")
    \item Si $ n $ est grand on fait un test asymptotique 
    \[
        \frac{S_n - E(S_n)}{\sqrt[]{Var(S_n)}} \to \mathcal{N}(0,1)
    .\]
    Sous $ H_0, E(S_n) = n/2, Var(S_n) = n/4 $ 
\end{itemize}

\underline{Nouveau cours du 31/03} \\
\textbf{Rappel} Résumé du test du signe
\begin{itemize}
    \item $ X_1, ..., X_n $, $ Y_1, ..., Y_n $ 2 échantillon appariés
    \item Hypothèse : la loi des $ Z_i = Y_i - X_i$ est continue
    \item Statistique de test : $ S_n = \sum_{i=1}^{n}\mathbbm{1}_{Z_i > 0} $ 
    \item HP: $ H_0 $ la médiane des $ Z_i $ vaut 0 : $ m_z = 0 $ contre $ H_1 = m_z > 0 $ ou $ m_z < 0 $ ou $ m_z \neq 0 $ 
    \item $ m_z > 0 $ signifie $ P(Y_i > X_i) > \frac{1}{2} $ 
    \item Sous $ H_0 : S_n \sim \mathcal{B}(n, \frac{1}{2}) $ 
    \item Sous $ H_1 : $ si $ m_z > 0  $ alors 
    \[
        p = P(z_i = 1) = P(Y_i > X_i) > \frac{1}{2}
    .\]
    Ainsi sou $ H_1, S_n \sim \mathcal{B}(n,p) $. Sous $ H_1, S_n $ prend de "grande valeur".
    \item Zone et seuil de rejet pour $ H_1 = m_z > 0 $. Soit $ h_{1-\alpha } $ le quantile d'ordre $ 1 - \alpha  $ de la $ \mathcal{B}(n ,\frac{1}{2}) $. Si $ S_n > h_{1 - \alpha } $  on rejette $ H_0 $ sinon on conserve $ H_0 $ 
    \item Remarque : \begin{enumerate}
        \item S'il existe un $ i $ tel que $ Z_i = 0 $. On est alors obligé d'exclure les données correspondantes. On adapte alors la valeur de $n$.
        \item Si $ n $ est petit, la table de la loi binomiale ne fournit pas de quantile exact. \begin{exmp}[]
            Si $ n=10 $ : \begin{itemize}
                \item $ P(\mathcal{B}(10, frac) \leq 7) = 0.945$
                \item $ P(\mathcal{B}(10, frac) \leq 8) = 0.989$
            \end{itemize}
            Si on veut faire un test à $ 5\% $ que faire ? On peut être tenté de regarder \begin{enumerate}
                \item $ \mathcal{R} = \{S \geq 8\} $ 
                \item $ \mathcal{R} = \{S \geq 9\} $ 
            \end{enumerate}
            Laquelle est la bonne pour avoir $ 5\% $ ?\begin{enumerate}
                \item Pour 1 : $ P_{H_0} (\mathcal{B}(10,\frac{1}{2}) \in \mathcal{R}) = 0.055 > 0.05 $ ce choix n'est pas acceptable pour un niveau $ \alpha  $ 
                \item On choisit la zone 2 
            \end{enumerate}
            En faite tout dépend de si on peut discuter avec les personnes, si c'est un text de loi, pas le choix pour bouger $ \alpha  $ de quelque dixième. Sinon penser à travailler avec la p valeur (qu'on va calculer plus tard).
        \end{exmp}
    \end{enumerate}
\end{itemize}

\begin{exmp}[propre d'un test du signe]
    Donnée de poids de patients après un changement d'alimentation\\
    \begin{table}[!h]
        \centering
        \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
        \hline
            i & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\ \hline
            Avant $X_i$ & 80 & 82 & 75 & 90 & 90 & 87 & 100 & 107 & 103 \\ \hline
            Après $Y_i$ & 79 & 83 & 75 & 81 & 95 & 86 & 101 & 105 & 100 \\ \hline
            Différence $Z_i = Y_i - X_i$ & -1 & 1 & 0 & -9 & -3 & -1 & 1 & -2 & -3 \\ \hline
        \end{tabular}
    \end{table}
    \begin{itemize}
        \item $ H_0 $ Pas d'effet du changement sur le poids, c'est à dire $ m_z = 0 $ 
        \item $ H_1 $ Le changement induit une diminution du poids $ m_z < 0 $ 
    \end{itemize}
    Ici on dispose de \textbf{deux échantillons appariés}. Pour une valeur de $ i, X_i $ et $ Y_i $ représentent le poids d'une personne avant et après le changement d'aliementation.

    Comme $ Z_3 = 0 $, on doit exclure $ i=3 $ 
    \[
        S = \sum_{i=1, i \neq 3}^{9} \mathbbm{1}_{Z_i > 0} = 2
    .\]

    Zone de rejet : $ \alpha = 5\% $ \\
    Sous $ H_1, S $ prend des plus petite valeur. Lecture de table : $ n=8 $ \begin{itemize}
        \item $ P(\mathcal{B}(2, \frac{1}{2}) \leq 1) = 0.035 $ 
        \item $ P(\mathcal{B}(2, \frac{1}{2}) \leq 2) = 0.145 $ 
    \end{itemize}
    Zone de rejet au niveau $ 5\% : \mathcal{R} = \{S \leq 1\}$. CCL : On conserve $ H_0 $.
\end{exmp}

\textbf{CCL} : Le test du signe ne prend en compte \textbf{que} le signe des $ Z_i $. C'est à la fois sa force \textbf{et} sa faiblesse. 

Dans de nombreux cas, il est délicat d'obtenir les amplitudes d'évolution. En revanche, dès qu'on les a et qu'elles ont du sens, il serait malavisé de ne pas utiliser. 

\subsection{Le test des rangs et signe de Wilcoxon}
\paragraph*{Données}: 2 échantillons appariées : $ X_1, \dots, X_n $ et $ Y_1, \dots, Y_n $. On pose $ Z_i = Y_i - X_i $ iid.
\begin{rem}[]
    \begin{itemize}
        \item Moyen rapide de voir si échantillons appariées : on a le même nombre $ n $.
        \item Attention le $ n $ dans les formules est le nombre de couple $ (X_i, Y_i) $ 
        \item En réalité si on regarde sur wikipedia, le test demande même pas iid
    \end{itemize}
\end{rem}

\paragraph*{Condition}:
\begin{itemize}
    \item La loi des $ Z_i $ est continue
    \item Les $ Z_i $ sont symétriques par rapport à leur médiane $ m $.
\end{itemize}
\begin{rem}[\textbf{Attention}].
    \begin{itemize}
        \item si $ E(\left| Z_i \right| ) < + \infty $ alors $ m = E(Z_i) $. 
        \item La condition sur la symétrie est délicate à vérifier. On se contentera de la supposer vrais dès que c'est raisonnable (typiquement en faisant un histogramme) \\
    \end{itemize}
\end{rem}

\begin{rem}[\textbf{Importante}]
    $ \left| Z_i \right|  $ et $ Signe(Z_i) = \frac{Z_i}{\left| Z_i \right| } = \begin{cases}
    1 &\text{si } Z_i > 0\\
    -1  &\text{si } Z_i < 0\\
    \end{cases} $ 

    \begin{proof}[Mini preuve: ]
        \begin{align*}
            \forall t > 0, P(sgn(Z) = 1 \text{ et } \left| Z \right| > T ) &= P(Z > t) \\ 
            &= \frac{1}{2} P(Z > t) + P(Z < -t) \\
            &= \frac{1}{2} P( \left| Z \right| > t) \\
            &= P(sgn(Z) = 1) P(\left| Z \right| > t)
        \end{align*}
        idem si $ sgn(Z) = -1 $. Bref : $ sgn(Z) $ et $ \left| Z \right|  $ sont indépendantes
    \end{proof}
\end{rem}

\paragraph*{Hypothèse}:
\begin{itemize}
    \item $ H_0 = m = 0$ 
    \item $ H_0 = m \neq 0 $ ou $ m > 0 $ ou $ m < 0 $ 
\end{itemize}
Si $ m=0 $ alors $ P(Y_i > X_i) = P(X_i < Y_i) = \frac{1}{2} $

\paragraph*{Statistique de test} Soit $ R_i $ le rang de $ \left| Z_i \right|  $ dans l'échantillon ordonnée issus de $ \left| Z_1 \right| , \dots, \left| Z_n \right|  $. \\ 
On pose 
\[
    W_n^+ = \sum_{i=1}^{n}\mathbbm{1}_{Z_i > 0}R_i \text{ la somme des rangs positifs}
.\]
et 
\[
    W_n^- = \sum_{i=1}^{n}\mathbbm{1}_{Z_i < 0}R_i
.\]
On remarque que 
\[
    W_n^+ + W_n^- = \sum_{i=1}^{n}R_i = 1 + 2 + \dots + n = \frac{n(n+1)}{2}
.\]

Sous $ H_0 $, $ W_n^+ $ et $ W_n^- $ ont la même loi, et celle-ci ne dépend pas de la loi des $ Z_i $. On l'appelle la loi de Wilcoxon. Elle est tabulée pour $ n $ entre 5 et 20.

Sous $ H_0 $, le vecteur $ (R_1, \dots, R_n) $ est une permutation aléatoire uniforme. Les $ \mathbbm{1}_{Z_i > 0} $ sont des variable de Bernouilli $ \frac{1}{2} $ indépendants des $ \left| Z_i \right| $ donc des $ R_i $. 

Ainsi, $ W_n^+ $ a même loi que $ \sum_{i=1}^{n}b_i R_i $ avec $ b_i $ iid. $ \mathcal{B}er(\frac{1}{2}) $ et $ R_i \bot b_i  $ et $ (R_1, \dots, R_n) $ permutation uniforme.

Sous $ H_0 $, la loi de $ W_n^+ $ et $ W_n^- $ est symétrique par rapport à leur moyenne $ \frac{n(n+1)}{4} $. 

\begin{figure}[!htbp]
    \centering
    \caption{Loi de Wilcoxon}
    \includegraphics*[width=.75\textwidth]{fig7.png}
\end{figure}

\paragraph*{Zone et seuil de rejet}
Sous $ H_0 $, $ W_n^+ $ et $ W_n^- $ sont tabulées

Sous $ H_1 = m > 0 $ alors $ W_n^+ $ prend de grandes valeurs et $ W_n^- $ de petites valeurs.

Soit $ h_\alpha  $ le quantile d'ordre $ \alpha  $ de la loi de Wilcoxon et $ h_{1 - \alpha } $ celui d'ordre $ 1 - \alpha  $ 

On peut utiliser comme zone de rejet $ W_n^+ \geq h_{1 - \alpha }$ ou $ W_n^- \leq  h_\alpha  $. En fait, il s'agit de la \textbf{même} condition, car, grâce à la symetrie de la loi de Wilcoxon.

Pour le test bilateral : \\
Si $ H_1 = m \neq 0 $ alors on utilise comme zone de rejet 
\[
    W_n^+ \geq h_{1 - \frac{\alpha}{2}} \text{ ou } W_n^+ \leq h_{\alpha /2} \text{ (tout avec } W_n^+ \text{)}
.\]
Ce qui peut se réécrire 
\[
    W_n^- \leq h_{\alpha /2} \text{ ou bien } W_n^+ \leq h_{\alpha /2} \text{ (tout avec un seul quantile }
.\]
avec les $ h_{\alpha /2} $ et $ h_{1-\alpha /2} $ sont les quantiles de la loi de Wilcoxon.

Il n'y a pas de différence entre les deux présentation c'est une affaire de goût personnel. 

\begin{exmp}[Retour sur l'exemple]
    A partir de l'exemple donnée dans le test du signe, on remplis le tableau. $ n=8 $ 

    \begin{table}[!h]
        \centering
        \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
        \hline
            $ \left| Z_i \right|  $  & 1 & 1 & 0 & 9 & 3 & 1 & 1 & 2 & 3 \\ \hline
            Signe & -  & + & X & - & - & - & + & -  & - \\ \hline
            Rang & 2.5 & 2.5 & X & 8 & 6.5 & 2.5 & 2.5 & 5 & 6.5 \\ \hline
        \end{tabular}
    \end{table}

    En cas d'égalité on attribue le rang moyen. 

    Dans l'exemple, les données sont appariées et \begin{itemize}
        \item $ H_0 = m = 0 $ 
        \item $ H_1 = m \neq 0 $ 
    \end{itemize}
    Ici, sous $ H_1 $ C'est $ W_n^+ $ qui prend des grandes valeurs. Calculons $ W_n^+ = 2.5 + 2.5 = 5 $ (automatiquement $ W_n^- = \frac{8*9}{2} - 5 = 36 -  5 = 31 $ ) 

    \paragraph*{Zone et seuil de rejet} :
    Dans la table pour $ n=8 $, on lit que $ P(W_n^+ \leq 5) \leq 0.05 $ et $ P(W_n^+ \leq 6) \geq 0.05 $. Ici on rejette $ H_0 $ si $ W_n^+ \leq 5 $. Or $ W_n^+ = 5 $ on rejette $ H_0 $ 
\end{exmp}

\paragraph*{Bilan sur le test de Wilcoxon}

Les avantages : 
\begin{itemize}
    \item Ce test est une alternative \textbf{non paramétrique} au test de Student pour échantillons appariées (on ne suppose pas que c'est gaussien)
    \item Il fonctionne \textbf{toujours} mieux que le test du signe 
    \item Si les données sont gaussiennes, le test de Wilcoxon est \textbf{à peine} moins bon que Students.
\end{itemize}
Bref : c'est un des meilleurs tests du cours pour les échantillons appariées

Les désavantages:
\begin{itemize}
    \item Hypothèse dure à vérifier, on se contente souvent de supposer que c'est applicable.
\end{itemize}

Version asymptotique : 

\[
    \frac{W_n^+ - \frac{n(n+1)}{4}}{\sqrt[]{\frac{n(n+1)(2n+1)}{24}}} \to ^{\alpha }_{n \to +\infty }\mathcal{N}(0,1)
.\]

\section{Remarques finales}
\subsection{La table de fisher}
La loi de Fisher à $ (d_1, d_2) $ degrés de liberté est la loi de $ \mathcal{F}_{d_1, d_2} = \frac{U_1 / d_1}{U_2 / d_2} $ où $ U_1 \sim \mathcal{X}^2(d_1), U_2 \sim \mathcal{X}^2 (d_2), U_1 \bot U_2 $. 

Ainsi $ F \sim \mathcal{Fisher}(d_1, d_2), \frac{1}{F} \sim \mathcal{Fisher}(d_2, d_1) $.\\
Cela implique : si on note $ h_{1-\alpha , d_1, d_2} $ le quantile d'ordre $ 1 - \alpha  $ de la loi $ \mathcal{Fisher}(d_1, d_2) $ alors 
\[
    h_{\alpha, d_1, d_2} = \frac{1}{h_{1-\alpha, d_2, d_1}}
.\]
\begin{exmp}[]
    \[
        h_{0.95, 10, 5} = 4.75 \Leftrightarrow h_{0.05, 10, 5} = \frac{1}{3.33} = \frac{1}{h_{0.95, 5 ,10}}
    .\]
\end{exmp}

\subsection{Table de Mann-Whitney et Wilcoxon}
Dans les deux cas, on utilise la symétrie par rapport à la médiane 
\begin{figure}[!htbp]
    \centering
    \includegraphics*[width=0.75\textwidth]{fig8.jpg}
    \caption{Loi de Mann-Whitney et Wilcoxon}
\end{figure}


\section{Test bonus}
Il est impossible de couvrir tous les tests de comparaison d'échantillons.

But : Devenir autonome et savoir aller chercher de nouveau test dans la littérature/sur internet. 

A chaque fois que vous verrez un test, j'aimerais que vous vous posiez les questions suivantes:
\begin{itemize}
    \item Combien d'échantillon ? Indépendant ? Liée ?
    \item Quelle loi sur les données ? Donnée gaussiennes ? Bernoulli ? Poisson ? \\
            Si vous n'avez aucune information sur la loi, regardez du côté des tests non paramétriques.
    \item Quelle taille d'échantillon ? Si les échantillons sont assez grands, vous pouvez faire des tests asymptotiques.
    \item Comment traduire ma question en hypothèses ? "Les échantillons sont-ils différents ?" C'est la manière de comprendre cette question qui vas \textbf{guider votre choix de test}. \\ 
        Comparaison de moyennes ? De variances ? De médianes ? De fonction de répartition ? 
\end{itemize}

\subsection{Le test d'indépendance de Pearson}
\paragraph*{Données:} \begin{itemize}
    \item $ X_1, \dots, X_n $ iid $ \mathcal{N}(m,\sigma ^2) $ 
    \item $ Y_1, \dots, Y_n $ iid $ \mathcal{N}(m,\sigma ^2) $ 
    \item Échantillon appariés et $ (X_i,Y_i) $ vecteur gaussien 
\end{itemize}

\paragraph*{Hypothèse}\begin{itemize}
    \item $ H_0 = X_i \bot Y_i $ ($ cor(X,Y) = 0 $ )
    \item $ H_0 = X_i \not\bot Y_i $ ($ cor(X,Y) \neq 0 $ )
\end{itemize}
Ici dans le cas particulier des gaussiens, le lien entre indépendance et corrélation est une équivalent.

\paragraph*{Statistique de test}
Soit $ R $ la corrélation empirique :
\begin{align*}
    R &= \frac{cov_n( (X_1, \dots, X_n), (Y_1, \dots, Y_n) )}{\sqrt[]{V_n^X*V_n^Y}} \\
    R_n &= \frac{\sum_{i=1}^{n} (X_i - \bar{X}_n) (Y_i - \bar{Y}_n)}{\sqrt[]{(\sum_{i=1}^{n} (X_i - \bar{X}_n)^2) \sum_{i=1}^{n} (Y_i - \bar{Y}_n)^2 }}
\end{align*}
$ R_n $ est un estimateur fortement consistant de $ cor(X_i, Y_i) $
\[
    D = \frac{R_n}{\sqrt[]{1 - R_n^2}} \sqrt[]{n-1}
.\]
\begin{itemize}
    \item Sous $ H_0, D \sim \mathcal{T}(n-2) $ 
    \item Sous $ H_1, D $ est grand en valeur absolue
\end{itemize}

\paragraph*{Zone et seuil de rejet}
On utilise des quantiles de la loi $ \mathcal{T}(n-2) $
\[
    \mathcal{R} = \{ D < h_{\alpha /2} \} \cup \{D > h_{1 - \alpha /2 }\}
.\]

\begin{rem}[]
    Il existe aussi des tests d'indépendance non-paramétriques : Ce sont les tests de Spearman et Kendall
\end{rem}

\subsection{Comparaison asymptotique de proportion}
\paragraph*{Motivation} On a plus jamais reparlé de variable de Bernoulli depuis le semestre dernier alors que c'est ce qu'on risque de rencontrer le plus souvent.
\paragraph*{Données}: \begin{itemize}
    \item $ X_1, \dots, X_n $ iid $ \mathcal{B}er(p_1) $ 
    \item $ Y_1, \dots, Y_n $ iid $ \mathcal{B}er(p_2) $ 
    \item $ (X_1, \dots, X_n) \bot (Y_1, \dots, Y_n) $
\end{itemize}

\paragraph*{Hypothèse}\begin{itemize}
    \item $ H_0 = p_1 = p_2 $ 
    \item $ H_1 = p_1 \neq p_2 $ ou $ p_1 < p_2 $ ou $ p_1 > p_2 $ 
\end{itemize}

\paragraph*{Statistique de test}
\[
    S_{n,m} = \frac{\bar{X}_n - \bar{Y}_n}{\sqrt[]{ \frac{\bar{X}_n (1 - \bar{X}_n)}{n}} + \frac{\bar{Y}_n (1 - \bar{Y}_n)}{n}}
.\]\begin{itemize}
    \item Sous $ H_0, S_{n,m} \to ^\mathcal{L}_{n,m \to +\infty } \mathcal{N}(0,1)$
    \item Sous $ H_1 $ \begin{itemize}
        \item Si $ p_1 > p_2, S_{n,m} \to_{n,m \to \infty } +\infty  $ 
        \item Si $ p_1 < p_2, S_{n,m} \to_{n,m \to \infty } +\infty  $ 
        \item Si $ p_1 \neq  p_2, S_{n,m} \to_{n,m \to \infty } +\infty  $ 
    \end{itemize}
\end{itemize}

\paragraph*{Zone et seuil de rejet}
\begin{itemize}
    \item $ H_1 : p_1 > p_2, \mathcal{R} = \{S > h_{1 -\alpha }\} $ 
    \item $ H_1 : p_1 < p_2, \mathcal{R} = \{S > h_{\alpha }\} $ 
    \item $ H_1 : p_1 \neq  p_2, \mathcal{R} = \{S > h_{\alpha /2}\} \cup \{S > h_{1 - \alpha /2}\} $ 
\end{itemize}
Où les $ h_{\alpha } $ sont des quantiles de la loi normale.

\section{Comparaison de $ K \geq 3 $ échantillons}
On dispose de $ K \geq 3 $ échantillons indépendants. On cherche à déterminer s'il existe une différence entre ces échantillons. \\
Cette "différence" peut se caractériser de plusieurs manières : différences de moyenne, médianes, variances, fonction de répartition
\begin{itemize}
    \item $ H_0 $ les échantillons ont la même caractéristiques
    \item $ H_1 $ Il existe au moins 2 populations qui diffèrent : $ \exists i,j \in \{1, \dots, K\} $ tq $ m_i \neq m_j $
\end{itemize}
Jusqu'à présent vous avez vu un seul test qui entre dans ce cadre : le $ \mathcal{X}^2 $ d'homogénéité. Le problème avec ce test est qu'il faut vraiment beaucoup de données.

\subsection{L'ANOVA à un facteur}
Vous avez vu dans le cours de modélisation statistique les aspects théorique du modèle linéaire gaussien. C'est \textbf{le} modèle le plus important en statistiques. \\
Ici la présentation ne traitera \textbf{que} des aspects pratiques. 

\paragraph*{Données} \begin{itemize}
    \item $ K $ échantillons \textbf{indépendants}
    \item $ X_1^{(1)}, \dots, X_{n_1}^{(1)} $ va iid $ \mathcal{N}(m, \sigma_{1} ^2) $ 
    \item $ X_1^{(2)}, \dots, X_{n_2}^{(2)} $ va iid $ \mathcal{N}(m, \sigma_{2} ^2) $ 
    \item $ X_1^{(K)}, \dots, X_{n_K}^{(K)} $ va iid $ \mathcal{N}(m, \sigma_{K} ^2) $
    \item On suppose de plus l'\textbf{homoscédasticité} : $ \sigma _1^2 = \dots = \sigma _K^2 $ 
\end{itemize}

\begin{rem}[]
    Si on pose $ n=n_1 + \dots + n_k $, cela corresponds à la vouloir expliquer les variables continues $ (Y_1, \dots, Y_n) = (X_1^{(1)}, \dots, X_{n_1}^{(1)}, \dots, X_{n+K}^{(K)})$ par la variable catégorielle à valeur dans $ \{1, \dots, K\} $ 
\end{rem}

Cette situation correspond à la situation classique : effet de "catégorie" sur "score continue". Exemple : \begin{itemize}
    \item Effet du département de résidence sur la taille
    \item Effet de la mention au bac sur le temps au 100m
\end{itemize}

\paragraph*{ANOVA en pratique}
\begin{itemize}
    \item $ H_0 = m_1 = \dots = m_k $ 
    \item $ H_1 = \exists i,j tq m_i \neq m_j $ 
\end{itemize}
La somme des carrés des écarts ($ \approx  $ variance sans diviser )
\[
    SCE^{(p)} = \sum_{i=1}^{n_p} (X_i^{(p)} - \bar{X}^{(p)})^2
.\]
\[
    \frac{SCE ^{(p)}}{n_p-1}= V^{(p)} \text{ variance empirique de l'échantillon p}
.\]

\begin{rem}[]
    Sous $ H_0, \frac{SCE^{(p)}}{\sigma ^2} \sim \mathcal{X}^2(n_p -1) $\\
    Sous $ H_1 $ aussi
\end{rem}

On calcule alors SCE ou SCT total ou intra
\[
    SCE^{totale}_{intra} = \sum_{p=1}^{K}SCE^{(p)}
.\]
\begin{rem}[]
    sous $ H_0 $ comme sous $ H_1 $ \begin{align*}
        \frac{SCE^{tot}}{\sigma ^2} &= \mathcal{X}^2(n_1 - 1) + \dots + \mathcal{X}^2(n_K -1) \text{ somme de khi deux indépendante} \\
        &= \mathcal{X}^2(n_1 + n_2 + \dots + n_K - K) \\
        &= \mathcal{X}^2(n - K)
    \end{align*}
\end{rem}

Enfin, on calcule 
\[
    \bar{X} = \text{ moyenne totale } = \frac{1}{n}\sum_{p=1}^{K} n_p \bar{X}^{(p)}
.\]
On calcule 
\[
    SCE_{inter} = \sum_{p=1}^{K} n_p (\bar{X}^{(p)} - \bar{X})^2
.\]
Sous $ H_0 : \frac{SCE_{inter}}{\sigma ^2} \sim \mathcal{X}^2(K-1)$  \\
Sous $ H_1, SCE_{inter} $ est \textbf{grand}

\paragraph*{Stat de test}
\[
    F = \frac{SCE_{inter} / (K-1)}{SCE_{intra}^{totale} / (n-K)}
.\]
Sous $ H_0 : F \sim \mathcal{F}(K_1, n-K) $ \\
Sous $ H_1, F $ prend de grande valeurs.

\paragraph*{Zone et seuil de rejet}
Au niveau $ \alpha  $, soit $ h_{1 - \alpha } $ le quantile d'ordre $ 1 - \alpha  $ de la loi $ \mathcal{F}(K-1, n-K) $. Si $ F > h_{1 - \alpha } $ on rejette $ H_0 $. 

\begin{rem}[]
    L'ANOVA porte ce nom à cause de la décomposition : 
    \begin{align*}
        & \sum_{p=1}^{K}\sum_{i=1}^{n_p}(X_i^{(p)} - \bar{X})^2 = SCE_{inter} + SCE^{totale}_{intra} \\
        = & \text{ Variance totale } = \text{ entre les groupes } + \text{ interne à chaque groupe}
    \end{align*}
    Analyse (de la décomposition) de la variance.
\end{rem}





\end{document}